{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dca42d1f-6a7c-43fd-96a0-44fb84926b72",
   "metadata": {},
   "source": [
    "# Multilingual searching using Amazon OpenSearch and Amazon SageMaker\n",
    "\n",
    "We will be deploying two embedding models to Amazon SageMaker.  These models will be used to create vectors of text in three languages (English, French and German).  These vectors will then be stored in Amazon OpenSearch and allow for semantic searches to be used across the language sets.\n",
    "\n",
    "The two models being used are:\n",
    "1. paraphrase-multilingual-MiniLM-L12-v2 (https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)\n",
    "2. paraphrase-multilingual-mpnet-base-v2 (https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2)\n",
    "\n",
    "We will also be deploying an Amazon OpenSearch cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2fcf2-0bee-4a6b-8bd5-20193f76fc05",
   "metadata": {},
   "source": [
    "#### Step 1. Install dependancies needed for this notebook.\n",
    "\n",
    "Ignore the ERROR about pip's dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "221d9f93-8687-40f9-9c0a-e1c2d39703f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests-aws4auth GitPython opensearch-py --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668fcdc-f216-49e0-8848-8225330dabc9",
   "metadata": {},
   "source": [
    "#### Step 2. Install git-lfs so that we can clone the model repos to our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1f925ad-3c10-4480-ba5a-77bb43ba710b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "amzn2-core                                               | 3.6 kB     00:00     \n",
      "amzn2extra-docker                                        | 2.9 kB     00:00     \n",
      "amzn2extra-kernel-5.10                                   | 3.0 kB     00:00     \n",
      "amzn2extra-livepatch                                     | 2.9 kB     00:00     \n",
      "amzn2extra-lustre                                        | 2.5 kB     00:00     \n",
      "amzn2extra-python3.8                                     | 2.9 kB     00:00     \n",
      "centos-extras                                            | 2.9 kB     00:00     \n",
      "copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxi | 3.3 kB     00:00     \n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "nvidia-container-toolkit/x86_64/signature                |  833 B     00:00     \n",
      "nvidia-container-toolkit/x86_64/signature                | 2.1 kB     00:02 !!! \n",
      "nvidia-container-toolkit/x86_64/primary                    |  18 kB   00:00     \n",
      "nvidia-container-toolkit                                                115/115\n",
      "63 packages excluded due to repository priority protections\n",
      "Package amazon-linux-extras-2.0.3-1.amzn2.noarch already installed and latest version\n",
      "Nothing to do\n",
      "Installing epel-release\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "Cleaning repos: amzn2-core amzn2extra-docker amzn2extra-epel\n",
      "              : amzn2extra-kernel-5.10 amzn2extra-livepatch amzn2extra-lustre\n",
      "              : amzn2extra-python3.8 centos-extras\n",
      "              : copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxidmap\n",
      "              : docker-ce-stable nvidia-container-toolkit\n",
      "39 metadata files removed\n",
      "17 sqlite files removed\n",
      "0 metadata files removed\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "amzn2-core                                               | 3.6 kB     00:00     \n",
      "amzn2extra-docker                                        | 2.9 kB     00:00     \n",
      "amzn2extra-epel                                          | 3.0 kB     00:00     \n",
      "amzn2extra-kernel-5.10                                   | 3.0 kB     00:00     \n",
      "amzn2extra-livepatch                                     | 2.9 kB     00:00     \n",
      "amzn2extra-lustre                                        | 2.5 kB     00:00     \n",
      "amzn2extra-python3.8                                     | 2.9 kB     00:00     \n",
      "centos-extras                                            | 2.9 kB     00:00     \n",
      "copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxi | 3.3 kB     00:00     \n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "nvidia-container-toolkit/x86_64/signature                |  833 B     00:00     \n",
      "nvidia-container-toolkit/x86_64/signature                | 2.1 kB     00:02 !!! \n",
      "(1/17): amzn2-core/2/x86_64/group_gz                       | 2.7 kB   00:00     \n",
      "(2/17): amzn2-core/2/x86_64/updateinfo                     | 1.0 MB   00:00     \n",
      "(3/17): amzn2extra-epel/2/x86_64/primary_db                | 1.8 kB   00:00     \n",
      "(4/17): amzn2extra-kernel-5.10/2/x86_64/updateinfo         |  93 kB   00:00     \n",
      "(5/17): amzn2extra-docker/2/x86_64/updateinfo              |  20 kB   00:00     \n",
      "(6/17): amzn2extra-epel/2/x86_64/updateinfo                |   76 B   00:00     \n",
      "(7/17): amzn2extra-docker/2/x86_64/primary_db              | 116 kB   00:00     \n",
      "(8/17): amzn2extra-livepatch/2/x86_64/updateinfo           |  21 kB   00:00     \n",
      "(9/17): amzn2extra-livepatch/2/x86_64/primary_db           |  61 kB   00:00     \n",
      "(10/17): amzn2extra-lustre/2/x86_64/primary_db             |  10 kB   00:00     \n",
      "(11/17): amzn2extra-python3.8/2/x86_64/updateinfo          | 8.2 kB   00:00     \n",
      "(12/17): amzn2extra-python3.8/2/x86_64/primary_db          |  71 kB   00:00     \n",
      "(13/17): centos-extras/primary_db                          | 253 kB   00:00     \n",
      "(14/17): copr:copr.fedorainfracloud.org:vbatts:shadow-util | 6.1 kB   00:00     \n",
      "(15/17): nvidia-container-toolkit/x86_64/primary           |  18 kB   00:00     \n",
      "(16/17): amzn2extra-kernel-5.10/2/x86_64/primary_db        |  32 MB   00:00     \n",
      "(17/17): amzn2-core/2/x86_64/primary_db                    |  72 MB   00:00     \n",
      "nvidia-container-toolkit                                                115/115\n",
      "64 packages excluded due to repository priority protections\n",
      "Resolving Dependencies\n",
      "--> Running transaction check\n",
      "---> Package epel-release.noarch 0:7-11 will be installed\n",
      "--> Finished Dependency Resolution\n",
      "\n",
      "Dependencies Resolved\n",
      "\n",
      "================================================================================\n",
      " Package              Arch           Version      Repository               Size\n",
      "================================================================================\n",
      "Installing:\n",
      " epel-release         noarch         7-11         amzn2extra-epel          15 k\n",
      "\n",
      "Transaction Summary\n",
      "================================================================================\n",
      "Install  1 Package\n",
      "\n",
      "Total download size: 15 k\n",
      "Installed size: 24 k\n",
      "Downloading packages:\n",
      "epel-release-7-11.noarch.rpm                               |  15 kB   00:00     \n",
      "Running transaction check\n",
      "Running transaction test\n",
      "Transaction test succeeded\n",
      "Running transaction\n",
      "Warning: RPMDB altered outside of yum.\n",
      "  Installing : epel-release-7-11.noarch                                     1/1 \n",
      "  Verifying  : epel-release-7-11.noarch                                     1/1 \n",
      "\n",
      "Installed:\n",
      "  epel-release.noarch 0:7-11                                                    \n",
      "\n",
      "Complete!\n",
      "  2  httpd_modules            available  \u001b[0m  [ =1.0  =stable ]\n",
      "  3  memcached1.5             available  \u001b[0m  \\\n",
      "        [ =1.5.1  =1.5.16  =1.5.17 ]\n",
      "  9  R3.4                     available  \u001b[0m  [ =3.4.3  =stable ]\n",
      " 10  rust1                    available  \u001b[0m  \\\n",
      "        [ =1.22.1  =1.26.0  =1.26.1  =1.27.2  =1.31.0  =1.38.0\n",
      "          =stable ]\n",
      " 18  libreoffice              available  \u001b[0m  \\\n",
      "        [ =5.0.6.2_15  =5.3.6.1  =stable ]\n",
      " 19  gimp                     available  \u001b[0m  [ =2.8.22 ]\n",
      " 20 â€ \u001b[94mdocker=latest            enabled    \u001b[0m  \\\n",
      "        [ =17.12.1  =18.03.1  =18.06.1  =18.09.9  =stable ]\n",
      " 21  mate-desktop1.x          available  \u001b[0m  \\\n",
      "        [ =1.19.0  =1.20.0  =stable ]\n",
      " 22  GraphicsMagick1.3        available  \u001b[0m  \\\n",
      "        [ =1.3.29  =1.3.32  =1.3.34  =stable ]\n",
      " 24  \u001b[94mepel=latest              enabled    \u001b[0m  [ =7.11  =stable ]\n",
      " 25  testing                  available  \u001b[0m  [ =1.0  =stable ]\n",
      " 26  ecs                      available  \u001b[0m  [ =stable ]\n",
      " 27 â€ corretto8                available  \u001b[0m  \\\n",
      "        [ =1.8.0_192  =1.8.0_202  =1.8.0_212  =1.8.0_222  =1.8.0_232\n",
      "          =1.8.0_242  =stable ]\n",
      " 32  lustre2.10               available  \u001b[0m  \\\n",
      "        [ =2.10.5  =2.10.8  =stable ]\n",
      " 34  lynis                    available  \u001b[0m  [ =stable ]\n",
      " 36  BCC                      available  \u001b[0m  [ =0.x  =stable ]\n",
      " 37  mono                     available  \u001b[0m  [ =5.x  =stable ]\n",
      " 38  nginx1                   available  \u001b[0m  [ =stable ]\n",
      " 40  mock                     available  \u001b[0m  [ =stable ]\n",
      " 43  \u001b[94mlivepatch=latest         enabled    \u001b[0m  [ =stable ]\n",
      " 44 \u001b[93m*\u001b[0m\u001b[94mpython3.8=latest         enabled    \u001b[0m  [ =stable ]\n",
      " 45  haproxy2                 available  \u001b[0m  [ =stable ]\n",
      " 46  collectd                 available  \u001b[0m  [ =stable ]\n",
      " 47  aws-nitro-enclaves-cli   available  \u001b[0m  [ =stable ]\n",
      " 48  R4                       available  \u001b[0m  [ =stable ]\n",
      "  _  kernel-5.4               available  \u001b[0m  [ =stable ]\n",
      " 50  selinux-ng               available  \u001b[0m  [ =stable ]\n",
      " 52  tomcat9                  available  \u001b[0m  [ =stable ]\n",
      " 53  unbound1.13              available  \u001b[0m  [ =stable ]\n",
      " 54 â€ mariadb10.5              available  \u001b[0m  [ =stable ]\n",
      " 55  \u001b[94mkernel-5.10=latest       enabled    \u001b[0m  [ =stable ]\n",
      " 56  redis6                   available  \u001b[0m  [ =stable ]\n",
      " 59 â€ postgresql13             available  \u001b[0m  [ =stable ]\n",
      " 60  mock2                    available  \u001b[0m  [ =stable ]\n",
      " 61  dnsmasq2.85              available  \u001b[0m  [ =stable ]\n",
      " 62  kernel-5.15              available  \u001b[0m  [ =stable ]\n",
      " 63 â€ postgresql14             available  \u001b[0m  [ =stable ]\n",
      " 64  firefox                  available  \u001b[0m  [ =stable ]\n",
      " 65  \u001b[94mlustre=latest            enabled    \u001b[0m  [ =stable ]\n",
      " 67  awscli1                  available  \u001b[0m  [ =stable ]\n",
      " 68 â€ php8.2                   available  \u001b[0m  [ =stable ]\n",
      " 69  dnsmasq                  available  \u001b[0m  [ =stable ]\n",
      " 70  unbound1.17              available  \u001b[0m  [ =stable ]\n",
      " 72  collectd-python3         available  \u001b[0m  [ =stable ]\n",
      "\u001b[93m*\u001b[0m Extra topic has reached end of support.\n",
      "â€  Note on end-of-support. Use 'info' subcommand.\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, langpacks, priorities,\n",
      "              : update-motd, versionlock\n",
      "================================== repo: epel ==================================\n",
      "[epel]\n",
      "async = True\n",
      "bandwidth = 0\n",
      "base_persistdir = /var/lib/yum/repos/x86_64/2\n",
      "baseurl = \n",
      "cache = 0\n",
      "cachedir = /var/cache/yum/x86_64/2/epel\n",
      "check_config_file_age = True\n",
      "compare_providers_priority = 80\n",
      "cost = 1000\n",
      "deltarpm_metadata_percentage = 100\n",
      "deltarpm_percentage = \n",
      "enabled = True\n",
      "enablegroups = True\n",
      "exclude = \n",
      "failovermethod = priority\n",
      "ftp_disable_epsv = False\n",
      "gpgcadir = /var/lib/yum/repos/x86_64/2/epel/gpgcadir\n",
      "gpgcakey = \n",
      "gpgcheck = True\n",
      "gpgdir = /var/lib/yum/repos/x86_64/2/epel/gpgdir\n",
      "gpgkey = file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\n",
      "hdrdir = /var/cache/yum/x86_64/2/epel/headers\n",
      "http_caching = all\n",
      "includepkgs = \n",
      "ip_resolve = \n",
      "keepalive = True\n",
      "keepcache = False\n",
      "mddownloadpolicy = sqlite\n",
      "mdpolicy = group:small\n",
      "mediaid = \n",
      "metadata_expire = 21600\n",
      "metadata_expire_filter = read-only:present\n",
      "metalink = https://mirrors.fedoraproject.org/metalink?repo=epel-7&arch=x86_64\n",
      "minrate = 0\n",
      "mirrorlist = \n",
      "mirrorlist_expire = 86400\n",
      "name = Extra Packages for Enterprise Linux 7 - x86_64\n",
      "old_base_cache_dir = \n",
      "password = \n",
      "persistdir = /var/lib/yum/repos/x86_64/2/epel\n",
      "pkgdir = /var/cache/yum/x86_64/2/epel/packages\n",
      "priority = 99\n",
      "proxy = False\n",
      "proxy_dict = \n",
      "proxy_password = \n",
      "proxy_username = \n",
      "repo_gpgcheck = False\n",
      "report_instanceid = False\n",
      "retries = 7\n",
      "skip_if_unavailable = False\n",
      "ssl_check_cert_permissions = True\n",
      "sslcacert = \n",
      "sslclientcert = \n",
      "sslclientkey = \n",
      "sslverify = True\n",
      "throttle = 0\n",
      "timeout = 5.0\n",
      "ui_id = epel/x86_64\n",
      "ui_repoid_vars = releasever,\n",
      "   basearch\n",
      "username = \n",
      "\n",
      "Loaded plugins: dkms-build-requires, extras_suggestions, kernel-livepatch,\n",
      "              : langpacks, priorities, update-motd, versionlock\n",
      "amzn2-core                                               | 3.6 kB     00:00     \n",
      "amzn2extra-docker                                        | 2.9 kB     00:00     \n",
      "amzn2extra-epel                                          | 3.0 kB     00:00     \n",
      "amzn2extra-kernel-5.10                                   | 3.0 kB     00:00     \n",
      "amzn2extra-livepatch                                     | 2.9 kB     00:00     \n",
      "amzn2extra-lustre                                        | 2.5 kB     00:00     \n",
      "amzn2extra-python3.8                                     | 2.9 kB     00:00     \n",
      "centos-extras                                            | 2.9 kB     00:00     \n",
      "copr:copr.fedorainfracloud.org:vbatts:shadow-utils-newxi | 3.3 kB     00:00     \n",
      "https://download.docker.com/linux/centos/2/x86_64/stable/repodata/repomd.xml: [Errno 14] HTTPS Error 404 - Not Found\n",
      "Trying other mirror.\n",
      "epel/x86_64/metalink                                     | 5.1 kB     00:00     \n",
      "epel                                                     | 4.3 kB     00:00     \n",
      "nvidia-container-toolkit/x86_64/signature                |  833 B     00:00     \n",
      "nvidia-container-toolkit/x86_64/signature                | 2.1 kB     00:02 !!! \n",
      "(1/3): epel/x86_64/group                                   | 399 kB   00:00     \n",
      "(2/3): epel/x86_64/updateinfo                              | 1.0 MB   00:00     \n",
      "(3/3): epel/x86_64/primary_db                              | 8.7 MB   00:00     \n",
      "294 packages excluded due to repository priority protections\n",
      "Resolving Dependencies\n",
      "--> Running transaction check\n",
      "---> Package git-lfs.x86_64 0:2.10.0-2.el7 will be installed\n",
      "--> Finished Dependency Resolution\n",
      "\n",
      "Dependencies Resolved\n",
      "\n",
      "================================================================================\n",
      " Package           Arch             Version                Repository      Size\n",
      "================================================================================\n",
      "Installing:\n",
      " git-lfs           x86_64           2.10.0-2.el7           epel           3.7 M\n",
      "\n",
      "Transaction Summary\n",
      "================================================================================\n",
      "Install  1 Package\n",
      "\n",
      "Total download size: 3.7 M\n",
      "Installed size: 13 M\n",
      "Downloading packages:\n",
      "warning: /var/cache/yum/x86_64/2/epel/packages/git-lfs-2.10.0-2.el7.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID 352c64e5: NOKEY\n",
      "Public key for git-lfs-2.10.0-2.el7.x86_64.rpm is not installed\n",
      "git-lfs-2.10.0-2.el7.x86_64.rpm                            | 3.7 MB   00:00     \n",
      "Retrieving key from file:///etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\n",
      "Importing GPG key 0x352C64E5:\n",
      " Userid     : \"Fedora EPEL (7) <epel@fedoraproject.org>\"\n",
      " Fingerprint: 91e9 7d7c 4a5e 96f1 7f3e 888f 6a2f aea2 352c 64e5\n",
      " Package    : epel-release-7-11.noarch (@amzn2extra-epel)\n",
      " From       : /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7\n",
      "Running transaction check\n",
      "Running transaction test\n",
      "Transaction test succeeded\n",
      "Running transaction\n",
      "  Installing : git-lfs-2.10.0-2.el7.x86_64                                  1/1 \n",
      "  Verifying  : git-lfs-2.10.0-2.el7.x86_64                                  1/1 \n",
      "\n",
      "Installed:\n",
      "  git-lfs.x86_64 0:2.10.0-2.el7                                                 \n",
      "\n",
      "Complete!\n",
      "Updated git hooks.\n",
      "Git LFS initialized.\n"
     ]
    }
   ],
   "source": [
    "!sudo yum install -y amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y \n",
    "!sudo yum-config-manager --enable epel\n",
    "!sudo yum install git-lfs -y\n",
    "!git lfs install"
   ]
  },
  
  {
   "cell_type": "markdown",
   "id": "fe3cb463",
   "metadata": {},
   "source": [
    "#### Step 4. Set S3 bucket, Sagemaker execution role ARN, and Opensearch Role ARN and as variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an IAM client\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "s3bucketName = "insert bucket name here"\n",
    "sageMakerExecutionRoleName = f\"{aws_region}-{aws_account_id}-SageMaker-Execution-demo-role\"\n",
    "sageMakerOpenSearchRoleName = f\"{aws_region}-{aws_account_id}-SageMaker-OpenSearch-demo-role\"\n",
    "\n",
    "response = iam_client.get_role(RoleName=sageMakerExecutionRoleName)\n",
    "sageMakerExecutionRoleArn = response['Role']['Arn']\n",
    "print(f\"SageMaker execution role ARN: {sageMakerExecutionRoleArn}\")\n",
    "\n",
    "response = iam_client.get_role(RoleName=sageMakerOpenSearchRoleName)\n",
    "sageMakerOpenSearchRoleArn = response['Role']['Arn']\n",
    "print(f\"OpenSearch role ARN: {sageMakerOpenSearchRoleArn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994e7a5-5918-4814-9d99-79d28203554d",
   "metadata": {},
   "source": [
    "#### Step 4.  Build the paraphrase-multilingual-MiniLM-L12-v2 model\n",
    "\n",
    "The frist model we will work with is the paraphrase-multilingual-MiniLM-L12-v2.\n",
    "\n",
    "This model can be found at https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
    "\n",
    "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "It has a max sequence length of 128, which means it truncates any text after 128 tokens.\n",
    "\n",
    "*note: this will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c48ec-208c-4e92-902b-61e9e5b313e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "import os.path\n",
    "import tarfile\n",
    "\n",
    "miniLMpath = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "# see if the model repo already exists, if not, clone it\n",
    "if not os.path.exists(miniLMpath):\n",
    "    git.Repo.clone_from('https://huggingface.co/sentence-transformers/' + miniLMpath, miniLMpath)\n",
    "\n",
    "# make sure the code directory exists for the inference.py file\n",
    "if not os.path.exists(miniLMpath + '/code'):\n",
    "    os.mkdir(miniLMpath + '/code')\n",
    "\n",
    "# overwrite the existing inference.py file if it exists, otherwise create it.\n",
    "with open(miniLMpath + '/code/inference.py', 'w') as inference:\n",
    "    inference.write(\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def model_fn(model_dir):\n",
    "# Load model from HuggingFace Hub\n",
    "    model = SentenceTransformer(model_dir)\n",
    "    return model\n",
    "def predict_fn(data, model):\n",
    "    results = model.encode(data)\n",
    "    returnVal = results.astype('float32')\n",
    "    return returnVal\n",
    "    \"\"\")\n",
    "\n",
    "with open(miniLMpath + '/code/requirements.txt', 'w') as inference:\n",
    "    inference.write(\"sentence_transformers\\n\")\n",
    "\n",
    "# create a tar file from the model\n",
    "\n",
    "with tarfile.open(miniLMpath + '/model.tar.gz', \"w:gz\") as tar:\n",
    "    tar.add(miniLMpath, \n",
    "            arcname=os.path.basename(''),\n",
    "            filter=lambda tarinfo: None if ('.git' in tarinfo.name or 'model.tar.gz' in tarinfo.name or 'model.safetensors' in tarinfo.name) else tarinfo)\n",
    "\n",
    "# upload to S3 bucket \n",
    "s3Client = boto3.client('s3')\n",
    "s3Client.upload_file(miniLMpath + '/model.tar.gz', s3BucketName, 'custom_inference/' + miniLMpath + '/model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21488a16-8dcf-45e8-b604-3e037d2322cd",
   "metadata": {},
   "source": [
    "#### Step 5.  Build the paraphrase-multilingual-mpnet-base-v2 model\n",
    "\n",
    "The second model we will work with is the paraphrase-multilingual-mpnet-base-v2.\n",
    "\n",
    "This model can be found at https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
    "\n",
    "This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "It has a max sequence length of 128, which means it truncates any text after 128 tokens.\n",
    "\n",
    "*note: this will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27443f9-e533-4a49-8ee5-b5ffacc15aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "import os.path\n",
    "import tarfile\n",
    "import boto3\n",
    "mpnetpath = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# see if the model repo already exists, if not, clone it\n",
    "if not os.path.exists(mpnetpath):\n",
    "    git.Repo.clone_from('https://huggingface.co/sentence-transformers/' + mpnetpath, mpnetpath)\n",
    "\n",
    "# make sure the code directory exists for the inference.py file\n",
    "if not os.path.exists(mpnetpath + '/code'):\n",
    "    os.mkdir(mpnetpath + '/code')\n",
    "\n",
    "# overwrite the existing inference.py file if it exists, otherwise create it.\n",
    "with open(mpnetpath + '/code/inference.py', 'w') as inference:\n",
    "    inference.write(\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def model_fn(model_dir):\n",
    "# Load model from HuggingFace Hub\n",
    "    model = SentenceTransformer(model_dir)\n",
    "    return model\n",
    "def predict_fn(data, model):\n",
    "    results = model.encode(data)\n",
    "    returnVal = results.astype('float32')\n",
    "    return returnVal\n",
    "    \"\"\")\n",
    "\n",
    "with open(mpnetpath + '/code/requirements.txt', 'w') as inference:\n",
    "    inference.write(\"sentence_transformers\\n\")\n",
    "\n",
    "# create a tar file from the model\n",
    "\n",
    "with tarfile.open(mpnetpath + '/model.tar.gz', \"w:gz\") as tar:\n",
    "    tar.add(mpnetpath, \n",
    "            arcname=os.path.basename(''),\n",
    "            filter=lambda tarinfo: None if ('.git' in tarinfo.name or 'model.tar.gz' in tarinfo.name or 'model.safetensors' in tarinfo.name) else tarinfo)\n",
    "\n",
    "# upload to S3 bucket \n",
    "s3Client = boto3.client('s3')\n",
    "s3Client.upload_file(mpnetpath + '/model.tar.gz', s3BucketName, 'custom_inference/' + mpnetpath + '/model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9212670-e2f5-45fa-8a5e-1c05387124fa",
   "metadata": {},
   "source": [
    "#### Step 6. Create the opensearch cluster\n",
    "\n",
    "This will create an OpenSearch cluster for use doing the workshop.\n",
    "\n",
    "Please update the code below, you will need to provide your own `username` and `password` below before running the code block. Please make sure your password is at minimum 8 characters long and includes atleast one uppercase letter, one lowercase, one digit, and one special character. Strong passwords are critical.\n",
    "\n",
    "You will also need to replace <<ip address>> with your IP address in the `AccessPolicies` below.  This will limit access to the cluster to only your IP address.\n",
    "\n",
    "*note: this will take several minutes (up to 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d99a39-e52b-4e26-b2f6-a780e7a08b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "username=\"<<username>>\"\n",
    "password=\"<<password>>\"\n",
    "ip_address=\"<<ip address>>\"\n",
    "\n",
    "openSearchClient = boto3.client('opensearch')\n",
    "stsClient = boto3.client('sts')\n",
    "service = 'aoss'\n",
    "region = 'us-east-1'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, service, session_token=credentials.token)\n",
    "                   \n",
    "AWS_ACCOUNT_ID = stsClient.get_caller_identity()[\"Account\"]\n",
    "\n",
    "domainName = 'multilingual-demo'\n",
    "\n",
    "access_policy = {\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"AWS\": \"*\"\n",
    "      },\n",
    "      \"Action\": \"es:*\",\n",
    "      \"Resource\": f\"arn:aws:es:{region}:{AWS_ACCOUNT_ID}:domain/multilingual-demo/*\",\n",
    "      \"Condition\": {\n",
    "        \"IpAddress\": {\n",
    "          \"aws:SourceIp\": f\"{ip_address}\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Principal\": {\n",
    "        \"AWS\": f\"arn:aws:iam::{AWS_ACCOUNT_ID}:role/{region}-{AWS_ACCOUNT_ID}-SageMaker-Execution-demo-role\"\n",
    "      },\n",
    "      \"Action\": [\n",
    "        \"es:ESHttpGet\",\n",
    "        \"es:ESHttpPut\"\n",
    "      ],\n",
    "      \"Resource\": f\"arn:aws:es:{region}:{AWS_ACCOUNT_ID}:domain/multilingual-demo/*\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "# Convert the policy to a JSON string\n",
    "access_policy_json = json.dumps(access_policy)\n",
    "\n",
    "createResponse = openSearchClient.create_domain(\n",
    "    DomainName=domainName,\n",
    "    EngineVersion='OpenSearch_2.13',\n",
    "    ClusterConfig={\n",
    "        'InstanceType': 't3.medium.search',\n",
    "        'InstanceCount': 1,\n",
    "    },\n",
    "    EBSOptions={\n",
    "        'EBSEnabled': True,\n",
    "        'VolumeType': 'gp3',\n",
    "        'VolumeSize': 100,\n",
    "        'Iops': 3500,\n",
    "        'Throughput': 125\n",
    "    },\n",
    "    AccessPolicies=access_policy_json,\n",
    "    IPAddressType='ipv4',\n",
    "    NodeToNodeEncryptionOptions={\n",
    "        'Enabled': True\n",
    "    },\n",
    "    DomainEndpointOptions={\n",
    "        'EnforceHTTPS': True,\n",
    "        'TLSSecurityPolicy': 'Policy-Min-TLS-1-2-PFS-2023-10',\n",
    "    },\n",
    "    AdvancedSecurityOptions={\n",
    "        'Enabled': True,\n",
    "        'InternalUserDatabaseEnabled': True,\n",
    "        'MasterUserOptions': {\n",
    "            'MasterUserName': username,\n",
    "            'MasterUserPassword': password,\n",
    "        },\n",
    "    },\n",
    "    EncryptionAtRestOptions={\n",
    "        'Enabled': True\n",
    "    }\n",
    ")\n",
    "\n",
    "domainState = 'Processing'\n",
    "while domainState != 'Active':\n",
    "    time.sleep(10)\n",
    "    status = openSearchClient.describe_domain_health(\n",
    "        DomainName=domainName\n",
    "    )\n",
    "    domainState = status['DomainState']\n",
    "\n",
    "domaininfo = openSearchClient.describe_domain(\n",
    "    DomainName=domainName\n",
    ")\n",
    "while True:\n",
    "    if 'Endpoint' in domaininfo['DomainStatus'].keys():\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(10)\n",
    "        domaininfo = openSearchClient.describe_domain(\n",
    "            DomainName=domainName\n",
    "        )\n",
    "\n",
    "host = 'https://' + domaininfo['DomainStatus']['Endpoint']\n",
    "print('Cluster URL: ' + host)\n",
    "print('Dashboard URL: ' + host + '/_dashboards')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1faa9-2607-40cf-8bbc-77748c39a745",
   "metadata": {},
   "source": [
    "#### Step 7. Add the SageMaker Execution role to OpenSearch\n",
    "\n",
    "For us to be able to interact with OpenSearch from the notebook we need to allow the SageMaker execution role that was created by the CloudFormationTemplate to perform actions in OpenSearch.\n",
    "\n",
    "Navigate to OpenSearch Dashboard (from the Dashboard URL created in step 6) and login using the username and password you provided above.  \n",
    "![Dashboard](images/1_dashboard.png)\n",
    "\n",
    "Then navigate to Security using the left hand menu.\n",
    "![Security](images/2_security.png)\n",
    "\n",
    "Next select **Roles** from the Security left hand menu.\n",
    "![Roles](images/3_roles.png)\n",
    "\n",
    "From the roles screen select **all_access**\n",
    "![all access](images/4_all_access.png)\n",
    "\n",
    "Select the **Mapped users** tab and then click on the **Manage mapping** button.\n",
    "![mapped users](images/5_mapped_users.png)\n",
    "\n",
    "Provide the **SageMaker Execution Role Arn** in step 3 from the CloudFormation Template output. (this was printed out in Step 3 above)\n",
    "![mapped users](images/6_backend_roles.png)\n",
    "\n",
    "Click on the **Map** button.\n",
    "\n",
    "Navigate back to the **Roles** screen by using the breadcrumb at the top of the dashboard.\n",
    "\n",
    "Search for the **ml_full_access** role and select it.\n",
    "![mapped users](images/7_ml_full_access.png)\n",
    "\n",
    "Select the **Mapped users** tab and then click on the **Manage mapping** button.\n",
    "![mapped users](images/8_ml_full_access_tabs.png)\n",
    "\n",
    "Provide the **SageMaker OpenSearch Role Arn** in step 3 from the CloudFormation Template output. (this was printed out in Step 3 above)\n",
    "![mapped users](images/9_add_role.png)\n",
    "\n",
    "Click on the **Map** button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a876258-5c16-4c09-a498-29529b0556ca",
   "metadata": {},
   "source": [
    "#### Step 8. Deploy the paraphrase-multilingual-MiniLM-L12-v2 model\n",
    "\n",
    "Now we will use the HuggingFace APIs to deploy the model as a SageMaker endpoint.\n",
    "\n",
    "*note: takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444a399-17f3-4cb4-bc54-8c533a6dcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = sageMakerExecutionRoleArn\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "MiniLML12v2 = f\"s3://{s3BucketName}/custom_inference/paraphrase-multilingual-MiniLM-L12-v2/model.tar.gz\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "MiniLML12v2_model = HuggingFaceModel(\n",
    "    model_data=MiniLML12v2,       # path to your model and script\n",
    "    role=role,                    # iam role with permissions to create an Endpoint\n",
    "    transformers_version=\"4.26\",  # transformers version used\n",
    "    pytorch_version=\"1.13\",        # pytorch version used\n",
    "    py_version='py39',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "MiniLML12v2_predictor = MiniLML12v2_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdba64d-a79b-4559-9898-579cf3faf96a",
   "metadata": {},
   "source": [
    "#### Step 9. Test the paraphrase-multilingual-MiniLM-L12-v2 endpoint\n",
    "\n",
    "Now we will test the newly created endpoint to see if it creates the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ed364-676d-4951-934d-ea4fded68927",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"query: It's nice to see the flowers bloom and hear the birds sing in the spring\"]\n",
    "\n",
    "res = MiniLML12v2_predictor.predict(data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b77848-3734-4c81-adae-e06d07d7df99",
   "metadata": {},
   "source": [
    "#### Step 10. Deploy the paraphrase-multilingual-mpnet-base-v2 model\n",
    "\n",
    "Now we will use the HuggingFace APIs to deploy the model as a SageMaker endpoint.\n",
    "\n",
    "*note: takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfb09a-d476-4b66-b476-a2acad8ef91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = sageMakerExecutionRoleArn\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "mpnetbasev2 = f\"s3://{s3BucketName}/custom_inference/paraphrase-multilingual-mpnet-base-v2/model.tar.gz\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "mpnetbasev2_model = HuggingFaceModel(\n",
    "    model_data=mpnetbasev2,       # path to your model and script\n",
    "    role=role,                    # iam role with permissions to create an Endpoint\n",
    "    transformers_version=\"4.26\",  # transformers version used\n",
    "    pytorch_version=\"1.13\",        # pytorch version used\n",
    "    py_version='py39',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "mpnetbasev2_predictor = mpnetbasev2_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b73b35d-5f37-46fb-a72b-c27914fe3362",
   "metadata": {},
   "source": [
    "#### Step 11. Test the paraphrase-multilingual-mpnet-base-v2 endpoint\n",
    "\n",
    "Now we will test the newly created endpoint to see if it creates the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526f8d4-6441-48d2-8ed5-a80a3d93f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"query: It's nice to see the flowers bloom and hear the birds sing in the spring\"]\n",
    "\n",
    "res = mpnetbasev2_predictor.predict(data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da11a4f-a2bf-430f-841d-87ef7b86d9ec",
   "metadata": {},
   "source": [
    "#### Step 12. Setup the commons connector\n",
    "\n",
    "We need to enable access control for the connector to talk to SageMaker.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7967a14-72f9-4aee-af72-96e9e18791cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "# Register repository\n",
    "path = '/_cluster/settings'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"persistent\": {\n",
    "        \"plugins.ml_commons.connector_access_control_enabled\": 'true'\n",
    "    }\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e806a20-d197-4fe6-86b9-0f6167b647e8",
   "metadata": {},
   "source": [
    "#### Step 13. Create the connector for the paraphrase-multilingual-MiniLM-L12-v2 model\n",
    "\n",
    "Now we will create the connector for the paraphrase-multilingual-MiniLM-L12-v2 model that we created in step 8.  This tells OpenSearch where to send the request to get the embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c3319-65b6-4774-9f37-615ee9d8c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register repository\n",
    "path = '/_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "  \"description\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"roleArn\": sageMakerOpenSearchRoleArn\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"service_name\": \"sagemaker\"\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"url\": \"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/\" + MiniLML12v2_predictor.endpoint_name + \"/invocations\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/json\"\n",
    "      },\n",
    "      \"request_body\": \"${parameters.input}\",\n",
    "      \"pre_process_function\": \"connector.pre_process.default.embedding\",\n",
    "      \"post_process_function\": \"connector.post_process.default.embedding\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "MiniLML12v2_connector_response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "MiniLML12v2_connector = MiniLML12v2_connector_response.json()[\"connector_id\"]\n",
    "print('Connector id: ' + MiniLML12v2_connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c018e6-295c-4941-854a-5806791a9da8",
   "metadata": {},
   "source": [
    "#### Step 14. Create the connector for the paraphrase-multilingual-MiniLM-L12-v2 model\n",
    "\n",
    "Now we will create the connector for the paraphrase-multilingual-MiniLM-L12-v2 model that we created in step 10.  This tells OpenSearch where to send the request to get the embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9687cc-541a-41e9-942e-83748e11a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register repository\n",
    "path = '/_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "  \"description\": \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"roleArn\": sageMakerOpenSearchRoleArn\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"service_name\": \"sagemaker\"\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"url\": \"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/\" + mpnetbasev2_predictor.endpoint_name + \"/invocations\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/json\"\n",
    "      },\n",
    "      \"request_body\": \"${parameters.input}\",\n",
    "      \"pre_process_function\": \"connector.pre_process.default.embedding\",\n",
    "      \"post_process_function\": \"connector.post_process.default.embedding\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "mpnetbasev2_connector_response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "mpnetbasev2_connector = mpnetbasev2_connector_response.json()[\"connector_id\"]\n",
    "print('Connector id: ' + mpnetbasev2_connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64808853-a3d6-4c8d-a6cb-79d5684c19d1",
   "metadata": {},
   "source": [
    "#### Step 14. Create the OpenSearch model group\n",
    "\n",
    "Next we create a model group to hold the models we are deploying to OpenSearch via SageMaker endpoints.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085a370-93d3-4574-9b3b-027ca26c5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_plugins/_ml/model_groups/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"external_models\",\n",
    "  \"description\": \"A model group for external models\"\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print('Group id: ' + response.json()['model_group_id'])\n",
    "group_id = response.json()['model_group_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0623536-d4ba-4904-b50a-e4f7ebf981a2",
   "metadata": {},
   "source": [
    "#### Step 15. Register the paraphrase-multilingual-MiniLM-L12-v2 model\n",
    "\n",
    "We now register the paraphrase-multilingual-MiniLM-L12-v2 model to the model group and the connector that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73106637-fdae-4792-99c8-a7f9341e5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"model_group_id\": group_id,\n",
    "    \"description\": \"multilingual vector model\",\n",
    "    \"connector_id\": MiniLML12v2_connector\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "minilm_model_id = response.json()['model_id']\n",
    "print('Model id: ' + minilm_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07c20a-f83f-49d4-9c92-31ba58e37c69",
   "metadata": {},
   "source": [
    "#### Step 16. Register the paraphrase-multilingual-mpnet-base-v2 model\n",
    "\n",
    "We now register the paraphrase-multilingual-mpnet-base-v2 model to the model group and the connector that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97532a46-0b38-4864-8b51-916ca3487252",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"model_group_id\": group_id,\n",
    "    \"description\": \"multilingual vector model\",\n",
    "    \"connector_id\": mpnetbasev2_connector\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "mpnet_model_id = response.json()['model_id']\n",
    "print('Model id: ' + mpnet_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82e41c-4e58-42f9-bc9c-646ea1150f14",
   "metadata": {},
   "source": [
    "#### Step 17. Deploy the paraphrase-multilingual-MiniLM-L12-v2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290f440-d630-49da-8556-1a0ea47981e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ minilm_model_id + '/_deploy'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32491a4f-c7af-4268-911f-6f147c930af0",
   "metadata": {},
   "source": [
    "#### Step 18. Deploy the paraphrase-multilingual-mpnet-base-v2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da8707-0e3f-403f-94f0-5de2ed402f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ mpnet_model_id + '/_deploy'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff8403-4865-4349-8924-56b45fcb0018",
   "metadata": {},
   "source": [
    "#### Step 19. Test the paraphrase-multilingual-MiniLM-L12-v2 model through OpenSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1de584-cea9-4415-95bd-97a4739dedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ minilm_model_id + '/_predict'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"parameters\": {\n",
    "    \"input\": [\"It's nice to see the flowers bloom and hear the birds sing in the spring\"]\n",
    "  }\n",
    "}\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d18b1-9cac-4314-9b4b-cb306281ee88",
   "metadata": {},
   "source": [
    "#### Step 20. Deploy the paraphrase-multilingual-mpnet-base-v2 model through OpenSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c275112-ffde-4d0f-8425-89641481a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ mpnet_model_id + '/_predict'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"parameters\": {\n",
    "    \"input\": [\"It's nice to see the flowers bloom and hear the birds sing in the spring\"]\n",
    "  }\n",
    "}\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411bceaa-c3b8-4765-ad66-5f912a504d93",
   "metadata": {},
   "source": [
    "#### Step 21. Create the paraphrase-multilingual-MiniLM-L12-v2 index pipeline\n",
    "\n",
    "Now we will create the pipeline for the index, this is how we tell OpenSearch to send the field(s) we wanted embeddings for to the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c1580-e6d8-4b37-bf00-76d5b8340d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_ingest/pipeline/paraphrase-multilingual-MiniLM-L12-v2-pipeline'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"description\": \"Sagemaker paraphrase-multilingual-MiniLM-L12-v2 Pipeline\",\n",
    "  \"processors\":[\n",
    "    {\n",
    "      \"text_chunking\": {\n",
    "        \"algorithm\": {\n",
    "          \"fixed_token_length\": {\n",
    "            \"token_limit\": 100,\n",
    "            \"overlap_rate\": 0.2,\n",
    "            \"tokenizer\": \"standard\"\n",
    "          }\n",
    "        },\n",
    "        \"field_map\": {\n",
    "          \"sentence\": \"sentence_chunk\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": minilm_model_id,\n",
    "        \"field_map\": {\n",
    "          \"sentence_chunk\": \"sentence_chunk_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c11fe8f-60a2-4cb5-815b-be0211ae5cb8",
   "metadata": {},
   "source": [
    "#### Step 22. Create the paraphrase-multilingual-mpnet-base-v2 index pipeline\n",
    "\n",
    "Now we will create the pipeline for the index, this is how we tell OpenSearch to send the field(s) we wanted embeddings for to the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761a6b4-69c3-4cb0-aee9-56edaf8ebc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_ingest/pipeline/paraphrase-multilingual-mpnet-base-v2-pipeline'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"description\": \"Sagemaker paraphrase-multilingual-mpnet-base-v2 Pipeline\",\n",
    "  \"processors\":[\n",
    "    {\n",
    "      \"text_chunking\": {\n",
    "        \"algorithm\": {\n",
    "          \"fixed_token_length\": {\n",
    "            \"token_limit\": 100,\n",
    "            \"overlap_rate\": 0.2,\n",
    "            \"tokenizer\": \"standard\"\n",
    "          }\n",
    "        },\n",
    "        \"field_map\": {\n",
    "          \"sentence\": \"sentence_chunk\"\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": mpnet_model_id,\n",
    "        \"field_map\": {\n",
    "          \"sentence_chunk\": \"sentence_chunk_embedding\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c3dad-004a-4238-9c90-f55b22fbd33b",
   "metadata": {},
   "source": [
    "#### Step 23. Create the paraphrase-multilingual-MiniLM-L12-v2 index\n",
    "\n",
    "Next we create the index using the pipeline.  You can see we have three fields in the index:\n",
    "1. sentence_vector - this is where the vector embedding will be stored when returned from SageMaker\n",
    "2. sentence - this is the native language sentence\n",
    "3. sentence_english - this is the english translation of the sentence, it is just here as I don't speak all three languages to see how well the model is doing :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c8fa8-c8c3-4903-815c-d8b7728a94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/paraphrase-multilingual-minilm-l12-v2-index'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"settings\": {\n",
    "    \"index.knn\": \"true\",\n",
    "    \"default_pipeline\": \"paraphrase-multilingual-MiniLM-L12-v2-pipeline\",\n",
    "    \"number_of_shards\": 4,\n",
    "    \"number_of_replicas\": 2\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"sentence_chunk_embedding\": {\n",
    "        \"type\": \"nested\",\n",
    "        \"properties\": {\n",
    "          \"knn\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 384,\n",
    "            \"method\": {\n",
    "              \"name\": \"hnsw\",\n",
    "              \"engine\": \"nmslib\",\n",
    "              \"space_type\": \"cosinesimil\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"sentence\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"sentence_english\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"id\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82bd16-4f8d-4428-a308-f4e262a685b0",
   "metadata": {},
   "source": [
    "#### Step 24. Create the paraphrase-multilingual-mpnet-base-v2 index\n",
    "\n",
    "Next we create the index using the pipeline.  You can see we have three fields in the index:\n",
    "1. sentence_vector - this is where the vector embedding will be stored when returned from SageMaker\n",
    "2. sentence - this is the native language sentence\n",
    "3. sentence_english - this is the english translation of the sentence, it is just here as I don't speak all three languages to see how well the model is doing :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd504081-22de-4dd6-b386-28ea29e6c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/paraphrase-multilingual-mpnet-base-v2-index'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"settings\": {\n",
    "    \"index.knn\": \"true\",\n",
    "    \"default_pipeline\": \"paraphrase-multilingual-mpnet-base-v2-pipeline\",\n",
    "    \"number_of_shards\": 4,\n",
    "    \"number_of_replicas\": 2\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"sentence_chunk_embedding\": {\n",
    "        \"type\": \"nested\",\n",
    "        \"properties\": {\n",
    "          \"knn\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": 768,\n",
    "            \"method\": {\n",
    "              \"name\": \"hnsw\",\n",
    "              \"engine\": \"nmslib\",\n",
    "              \"space_type\": \"cosinesimil\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"sentence\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"sentence_english\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"id\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcf080-711d-486e-80c3-0fd4dc503b23",
   "metadata": {},
   "source": [
    "#### Step 25. Send the sentences to the paraphrase-multilingual-minilm-l12-v2-index OpenSearch index for indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb346b-05b0-43f5-a29e-5cd3da53f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "hostname = host[8:len(host)] \n",
    "region = 'us-east-1'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': hostname, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")\n",
    "\n",
    "with open(\"english.json\", 'r') as file:\n",
    "    english = file.read()\n",
    "\n",
    "with open(\"french.json\", 'r') as file:\n",
    "    french = file.read()\n",
    "\n",
    "with open(\"german.json\", 'r') as file:\n",
    "    german = file.read()\n",
    "docs = english + french + german\n",
    "\n",
    "results = client.bulk(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60318d5-c430-46e9-9d39-e3864536e08e",
   "metadata": {},
   "source": [
    "#### Step 26. Send the sentences to the paraphrase-multilingual-mpnet-base-v2-index OpenSearch index for indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bc63b-a640-49aa-bf3f-a3ef429853b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': hostname, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")\n",
    "\n",
    "with open(\"english.json\", 'r') as file:\n",
    "    english = file.read()\n",
    "\n",
    "with open(\"french.json\", 'r') as file:\n",
    "    french = file.read()\n",
    "\n",
    "with open(\"german.json\", 'r') as file:\n",
    "    german = file.read()\n",
    "docs = english + french + german\n",
    "\n",
    "docs = docs.replace(\"paraphrase-multilingual-minilm-l12-v2-index\", \"paraphrase-multilingual-mpnet-base-v2-index\")\n",
    "\n",
    "results = client.bulk(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af191c",
   "metadata": {},
   "source": [
    "#### Step 27. Test the indexes\n",
    "\n",
    "From the OpenSearch dashboard, click on the hamburger menu at the top left and select **Search Relevance**\n",
    "\n",
    "![Search Releveance screen](images/search_relevance.png)\n",
    "\n",
    "On the Search relevance screen select **paraphrase-multilingual-minilm-l2-v2-index** for the Query 1 **index** and the following code in the Query 1 **Query** text box.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"nested\": {\n",
    "      \"score_mode\": \"max\",\n",
    "      \"path\": \"sentence_chunk_embedding\",\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"sentence_chunk_embedding.knn\": {\n",
    "            \"query_text\": \"%SearchText%\",\n",
    "            \"model_id\": \"<model_id from step 15>\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"size\": \"30\",\n",
    "  \"_source\": [\"sentence\", \"sentence_english\"]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "select **paraphrase-multilingual-mpneet-base-v2-index** for the Query 2 **index** and the following code in the Query 2 **Query** text box.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"nested\": {\n",
    "      \"score_mode\": \"max\",\n",
    "      \"path\": \"sentence_chunk_embedding\",\n",
    "      \"query\": {\n",
    "        \"neural\": {\n",
    "          \"sentence_chunk_embedding.knn\": {\n",
    "            \"query_text\": \"%SearchText%\",\n",
    "            \"model_id\": \"<model_id from step 16>\"\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"size\": \"30\",\n",
    "  \"_source\": [\"sentence\", \"sentence_english\"]\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "Some sample query terms are:\n",
    "- mechanical parts\n",
    "- the season after winter\n",
    "- moving quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee054c32-ca65-4e86-9de3-e1f6b894243f",
   "metadata": {},
   "source": [
    "#### Step 28. Cleanup resources\n",
    "\n",
    "We will now delete the following:\n",
    "1. OpenSearch Cluster\n",
    "2. paraphrase-multilingual-MiniLM-L12-v2 endpoint\n",
    "3. paraphrase-multilingual-mpnet-base-v2 endpoint\n",
    "4. Models stored in S3\n",
    "5. Delete the S3 bucket used to store your models after emptying the objects and folders within the bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2afe9-4a0d-44b7-823b-267adbbf42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "openSearchClient.delete_domain(\n",
    "    DomainName='multilingual-demo'\n",
    ")\n",
    "\n",
    "MiniLML12v2_predictor.delete_model()\n",
    "MiniLML12v2_predictor.delete_endpoint()\n",
    "\n",
    "mpnetbasev2_predictor.delete_model()\n",
    "mpnetbasev2_predictor.delete_endpoint()\n",
    "\n",
    "s3Client.delete_object(Bucket=s3BucketName, Key='custom_inference/' + path + '/model.tar.gz')\n",
    "s3Client.delete_object(Bucket=s3BucketName, Key='custom_inference/' + mpnetpath + '/model.tar.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
