{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dca42d1f-6a7c-43fd-96a0-44fb84926b72",
   "metadata": {},
   "source": [
    "# Multilingual searching using Amazon OpenSearch and Amazon SageMaker\n",
    "\n",
    "We will be deploying two embedding models to Amazon SageMaker.  These models will be used to create vectors of text in three languages (English, French and German).  These vectors will then be stored in Amazon OpenSearch and allow for semantic searches to be used across the language sets.\n",
    "\n",
    "The two models being used are:\n",
    "1. paraphrase-multilingual-MiniLM-L12-v2 (https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2)\n",
    "2. paraphrase-multilingual-mpnet-base-v2 (https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2)\n",
    "\n",
    "We will also be deploying an Amazon OpenSearch cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b2fcf2-0bee-4a6b-8bd5-20193f76fc05",
   "metadata": {},
   "source": [
    "#### Step 1. Install dependancies needed for this notebook.\n",
    "\n",
    "Ignore the ERROR about pip's dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d9f93-8687-40f9-9c0a-e1c2d39703f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker requests-aws4auth GitPython opensearch-py --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4668fcdc-f216-49e0-8848-8225330dabc9",
   "metadata": {},
   "source": [
    "#### Step 2. Install git-lfs so that we can clone the model repos to our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f925ad-3c10-4480-ba5a-77bb43ba710b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo yum install -y amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y \n",
    "!sudo yum-config-manager --enable epel\n",
    "!sudo yum install git-lfs -y\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd653b6-d67f-4e5f-9055-a05c8147ec3d",
   "metadata": {},
   "source": [
    "#### Step 3.  We need to provide the name of the S3 bucket that was created by the CloudFormation template to store the models for us.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47703ced-6e6d-424b-b8c9-3ad8e0c0ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# need to get S3 bucket name from CFT output here\n",
    "cf_client = boto3.client('cloudformation')\n",
    "cf_response = cf_client.describe_stacks(StackName='OpenSearchSageMakerDemo')\n",
    "\n",
    "for output in cf_response['Stacks'][0]['Outputs']:\n",
    "    value = output['OutputValue']\n",
    "    if output['OutputKey'] == 'S3BucketName':\n",
    "        s3BucketName = value\n",
    "    elif output['OutputKey'] == 'SageMakerExecutionRoleArn':\n",
    "        sageMakerExecutionRoleArn = value\n",
    "    elif output['OutputKey'] == 'SageMakerOpenSearchRoleArn':\n",
    "        sageMakerOpenSearchRoleArn = value\n",
    "\n",
    "print('S3 Bucket Name: ' + s3BucketName)\n",
    "print('SageMaker Execution Role Arn: ' + sageMakerExecutionRoleArn)\n",
    "print('SageMaker OpenSearch Role Arn: ' + sageMakerOpenSearchRoleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c994e7a5-5918-4814-9d99-79d28203554d",
   "metadata": {},
   "source": [
    "#### Step 4.  Build the paraphrase-multilingual-MiniLM-L12-v2 model\n",
    "\n",
    "The frist model we will work with is the paraphrase-multilingual-MiniLM-L12-v2.\n",
    "\n",
    "This model can be found at https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
    "\n",
    "This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "It has a max sequence length of 128, which means it truncates any text after 128 tokens.\n",
    "\n",
    "*note: this will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56c48ec-208c-4e92-902b-61e9e5b313e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "import os.path\n",
    "import tarfile\n",
    "\n",
    "miniLMpath = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    "\n",
    "# see if the model repo already exists, if not, clone it\n",
    "if not os.path.exists(miniLMpath):\n",
    "    git.Repo.clone_from('https://huggingface.co/sentence-transformers/' + miniLMpath, miniLMpath)\n",
    "\n",
    "# make sure the code directory exists for the inference.py file\n",
    "if not os.path.exists(miniLMpath + '/code'):\n",
    "    os.mkdir(miniLMpath + '/code')\n",
    "\n",
    "# overwrite the existing inference.py file if it exists, otherwise create it.\n",
    "with open(miniLMpath + '/code/inference.py', 'w') as inference:\n",
    "    inference.write(\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def model_fn(model_dir):\n",
    "# Load model from HuggingFace Hub\n",
    "    model = SentenceTransformer(model_dir)\n",
    "    return model\n",
    "def predict_fn(data, model):\n",
    "    results = model.encode(data)\n",
    "    returnVal = results.astype('float32')\n",
    "    return returnVal\n",
    "    \"\"\")\n",
    "\n",
    "with open(miniLMpath + '/code/requirements.txt', 'w') as inference:\n",
    "    inference.write(\"sentence_transformers\\n\")\n",
    "\n",
    "# create a tar file from the model\n",
    "\n",
    "with tarfile.open(miniLMpath + '/model.tar.gz', \"w:gz\") as tar:\n",
    "    tar.add(miniLMpath, \n",
    "            arcname=os.path.basename(''),\n",
    "            filter=lambda tarinfo: None if ('.git' in tarinfo.name or 'model.tar.gz' in tarinfo.name or 'model.safetensors' in tarinfo.name) else tarinfo)\n",
    "\n",
    "# upload to S3 bucket \n",
    "s3Client = boto3.client('s3')\n",
    "s3Client.upload_file(miniLMpath + '/model.tar.gz', s3BucketName, 'custom_inference/' + miniLMpath + '/model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21488a16-8dcf-45e8-b604-3e037d2322cd",
   "metadata": {},
   "source": [
    "#### Step 5.  Build the paraphrase-multilingual-mpnet-base-v2 model\n",
    "\n",
    "The second model we will work with is the paraphrase-multilingual-mpnet-base-v2.\n",
    "\n",
    "This model can be found at https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
    "\n",
    "This is a sentence-transformers model: It maps sentences & paragraphs to a 768 dimensional dense vector space and can be used for tasks like clustering or semantic search.\n",
    "\n",
    "It has a max sequence length of 128, which means it truncates any text after 128 tokens.\n",
    "\n",
    "*note: this will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27443f9-e533-4a49-8ee5-b5ffacc15aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "import os.path\n",
    "import tarfile\n",
    "import boto3\n",
    "mpnetpath = 'paraphrase-multilingual-mpnet-base-v2'\n",
    "\n",
    "# see if the model repo already exists, if not, clone it\n",
    "if not os.path.exists(mpnetpath):\n",
    "    git.Repo.clone_from('https://huggingface.co/sentence-transformers/' + mpnetpath, mpnetpath)\n",
    "\n",
    "# make sure the code directory exists for the inference.py file\n",
    "if not os.path.exists(mpnetpath + '/code'):\n",
    "    os.mkdir(mpnetpath + '/code')\n",
    "\n",
    "# overwrite the existing inference.py file if it exists, otherwise create it.\n",
    "with open(mpnetpath + '/code/inference.py', 'w') as inference:\n",
    "    inference.write(\"\"\"\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def model_fn(model_dir):\n",
    "# Load model from HuggingFace Hub\n",
    "    model = SentenceTransformer(model_dir)\n",
    "    return model\n",
    "def predict_fn(data, model):\n",
    "    results = model.encode(data)\n",
    "    returnVal = results.astype('float32')\n",
    "    return returnVal\n",
    "    \"\"\")\n",
    "\n",
    "with open(mpnetpath + '/code/requirements.txt', 'w') as inference:\n",
    "    inference.write(\"sentence_transformers\\n\")\n",
    "\n",
    "# create a tar file from the model\n",
    "\n",
    "with tarfile.open(mpnetpath + '/model.tar.gz', \"w:gz\") as tar:\n",
    "    tar.add(mpnetpath, \n",
    "            arcname=os.path.basename(''),\n",
    "            filter=lambda tarinfo: None if ('.git' in tarinfo.name or 'model.tar.gz' in tarinfo.name or 'model.safetensors' in tarinfo.name) else tarinfo)\n",
    "\n",
    "# upload to S3 bucket \n",
    "s3Client = boto3.client('s3')\n",
    "s3Client.upload_file(mpnetpath + '/model.tar.gz', s3BucketName, 'custom_inference/' + mpnetpath + '/model.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9212670-e2f5-45fa-8a5e-1c05387124fa",
   "metadata": {},
   "source": [
    "#### Step 6. Create the opensearch cluster\n",
    "\n",
    "This will create an OpenSearch cluster for use doing the workshop.\n",
    "\n",
    "Please update the code below, you will need to provide your own `username` and `password` on lines 5 and 6 below before running the code block.\n",
    "\n",
    "*note: this will take several minutes (up to 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d99a39-e52b-4e26-b2f6-a780e7a08b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "username= \"<Change Me>\"\n",
    "password=\"<Change Me>\"\n",
    "\n",
    "openSearchClient = boto3.client('opensearch')\n",
    "stsClient = boto3.client('sts')\n",
    "service = 'aoss'\n",
    "region = 'us-east-1'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, service, session_token=credentials.token)\n",
    "                   \n",
    "AWS_ACCOUNT_ID = stsClient.get_caller_identity()[\"Account\"]\n",
    "\n",
    "domainName = 'multilingual-demo'\n",
    "\n",
    "createResponse = openSearchClient.create_domain(\n",
    "    DomainName=domainName,\n",
    "    EngineVersion='OpenSearch_2.11',\n",
    "    ClusterConfig={\n",
    "        'InstanceType': 't3.medium.search',\n",
    "        'InstanceCount': 3,\n",
    "        'DedicatedMasterEnabled': True,\n",
    "        'DedicatedMasterType': 't3.medium.search',\n",
    "        'DedicatedMasterCount': 3,\n",
    "    },\n",
    "    EBSOptions={\n",
    "        'EBSEnabled': True,\n",
    "        'VolumeType': 'gp3',\n",
    "        'VolumeSize': 100,\n",
    "        'Iops': 3500,\n",
    "        'Throughput': 125\n",
    "    },\n",
    "    AccessPolicies=f'{{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{{\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{{\\\"AWS\\\":\\\"*\"}},\\\"Action\\\":\\\"es:*\\\",\\\"Resource\\\":\\\"arn:aws:es:us-east-1:{AWS_ACCOUNT_ID}:domain\\/{domainName}\\/*\\\"}}]}}',\n",
    "    IPAddressType='ipv4',\n",
    "    NodeToNodeEncryptionOptions={\n",
    "        'Enabled': True\n",
    "    },\n",
    "    DomainEndpointOptions={\n",
    "        'EnforceHTTPS': True,\n",
    "        'TLSSecurityPolicy': 'Policy-Min-TLS-1-2-PFS-2023-10',\n",
    "    },\n",
    "    AdvancedSecurityOptions={\n",
    "        'Enabled': True,\n",
    "        'InternalUserDatabaseEnabled': True,\n",
    "        'MasterUserOptions': {\n",
    "            'MasterUserName': username,\n",
    "            'MasterUserPassword': password,\n",
    "        },\n",
    "    },\n",
    "    EncryptionAtRestOptions={\n",
    "        'Enabled': True\n",
    "    }\n",
    ")\n",
    "\n",
    "domainState = 'Processing'\n",
    "while domainState != 'Active':\n",
    "    time.sleep(10)\n",
    "    status = openSearchClient.describe_domain_health(\n",
    "        DomainName=domainName\n",
    "    )\n",
    "    domainState = status['DomainState']\n",
    "\n",
    "domaininfo = openSearchClient.describe_domain(\n",
    "    DomainName=domainName\n",
    ")\n",
    "while True:\n",
    "    if 'Endpoint' in domaininfo['DomainStatus'].keys():\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(10)\n",
    "        domaininfo = openSearchClient.describe_domain(\n",
    "            DomainName=domainName\n",
    "        )\n",
    "\n",
    "host = 'https://' + domaininfo['DomainStatus']['Endpoint']\n",
    "print('Cluster URL: ' + host)\n",
    "print('Dashboard URL: ' + host + '/_dashboards')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a1faa9-2607-40cf-8bbc-77748c39a745",
   "metadata": {},
   "source": [
    "#### Step 7. Add the SageMaker Execution role to OpenSearch\n",
    "\n",
    "For us to be able to interact with OpenSearch from the notebook we need to allow the SageMaker execution role that was created by the CloudFormationTemplate to perform actions in OpenSearch.\n",
    "\n",
    "Navigate to OpenSearch Dashboard (from the Dashboard URL created in step 6) and login using the username and password you provided above.  \n",
    "![Dashboard](images/1_dashboard.png)\n",
    "\n",
    "Then navigate to Security using the left hand menu.\n",
    "![Security](images/2_security.png)\n",
    "\n",
    "Next select **Roles** from the Security left hand menu.\n",
    "![Roles](images/3_roles.png)\n",
    "\n",
    "From the roles screen select **all_access**\n",
    "![all access](images/4_all_access.png)\n",
    "\n",
    "Select the **Mapped users** tab and then click on the **Manage mapping** button.\n",
    "![mapped users](images/5_mapped_users.png)\n",
    "\n",
    "Provide the **SageMaker Execution Role Arn** in step 3 from the CloudFormation Template output. (this was printed out in Step 3 above)\n",
    "![mapped users](images/6_backend_roles.png)\n",
    "\n",
    "Click on the **Map** button.\n",
    "\n",
    "Navigate back to the **Roles** screen by using the breadcrumb at the top of the dashboard.\n",
    "\n",
    "Search for the **ml_full_access** role and select it.\n",
    "![mapped users](images/7_ml_full_access.png)\n",
    "\n",
    "Select the **Mapped users** tab and then click on the **Manage mapping** button.\n",
    "![mapped users](images/8_ml_full_access_tabs.png)\n",
    "\n",
    "Provide the **SageMaker OpenSearch Role Arn** in step 3 from the CloudFormation Template output. (this was printed out in Step 3 above)\n",
    "![mapped users](images/9_add_role.png)\n",
    "\n",
    "Click on the **Map** button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a876258-5c16-4c09-a498-29529b0556ca",
   "metadata": {},
   "source": [
    "#### Step 8. Deploy the paraphrase-multilingual-MiniLM-L12-v2 model\n",
    "\n",
    "Now we will use the HuggingFace APIs to deploy the model as a SageMaker endpoint.\n",
    "\n",
    "*note: takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444a399-17f3-4cb4-bc54-8c533a6dcb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = sageMakerExecutionRoleArn\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "MiniLML12v2 = f\"s3://{s3BucketName}/custom_inference/paraphrase-multilingual-MiniLM-L12-v2/model.tar.gz\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "MiniLML12v2_model = HuggingFaceModel(\n",
    "    model_data=MiniLML12v2,       # path to your model and script\n",
    "    role=role,                    # iam role with permissions to create an Endpoint\n",
    "    transformers_version=\"4.26\",  # transformers version used\n",
    "    pytorch_version=\"1.13\",        # pytorch version used\n",
    "    py_version='py39',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "MiniLML12v2_predictor = MiniLML12v2_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name = 'paraphrase-multilingual-MiniLM-L12-v2'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdba64d-a79b-4559-9898-579cf3faf96a",
   "metadata": {},
   "source": [
    "#### Step 9. Test the paraphrase-multilingual-MiniLM-L12-v2 endpoint\n",
    "\n",
    "Now we will test the newly created endpoint to see if it creates the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388ed364-676d-4951-934d-ea4fded68927",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"query: It's nice to see the flowers bloom and hear the birds sing in the spring\"]\n",
    "\n",
    "res = MiniLML12v2_predictor.predict(data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b77848-3734-4c81-adae-e06d07d7df99",
   "metadata": {},
   "source": [
    "#### Step 10. Deploy the paraphrase-multilingual-mpnet-base-v2 model\n",
    "\n",
    "Now we will use the HuggingFace APIs to deploy the model as a SageMaker endpoint.\n",
    "\n",
    "*note: takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfb09a-d476-4b66-b476-a2acad8ef91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = sageMakerExecutionRoleArn\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "mpnetbasev2 = f\"s3://{s3BucketName}/custom_inference/paraphrase-multilingual-mpnet-base-v2/model.tar.gz\"\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "mpnetbasev2_model = HuggingFaceModel(\n",
    "    model_data=mpnetbasev2,       # path to your model and script\n",
    "    role=role,                    # iam role with permissions to create an Endpoint\n",
    "    transformers_version=\"4.26\",  # transformers version used\n",
    "    pytorch_version=\"1.13\",        # pytorch version used\n",
    "    py_version='py39',            # python version used\n",
    ")\n",
    "\n",
    "# deploy the endpoint endpoint\n",
    "mpnetbasev2_predictor = mpnetbasev2_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    endpoint_name = 'paraphrase-multilingual-mpnet-base-v2'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b73b35d-5f37-46fb-a72b-c27914fe3362",
   "metadata": {},
   "source": [
    "#### Step 11. Test the paraphrase-multilingual-mpnet-base-v2 endpoint\n",
    "\n",
    "Now we will test the newly created endpoint to see if it creates the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7526f8d4-6441-48d2-8ed5-a80a3d93f080",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"query: It's nice to see the flowers bloom and hear the birds sing in the spring\"]\n",
    "\n",
    "res = mpnetbasev2_predictor.predict(data)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da11a4f-a2bf-430f-841d-87ef7b86d9ec",
   "metadata": {},
   "source": [
    "#### Step 12. Setup the commons connector\n",
    "\n",
    "We need to enable access control for the connector to talk to SageMaker.\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7967a14-72f9-4aee-af72-96e9e18791cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "# Register repository\n",
    "path = '/_cluster/settings'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"persistent\": {\n",
    "        \"plugins.ml_commons.connector_access_control_enabled\": 'true'\n",
    "    }\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e806a20-d197-4fe6-86b9-0f6167b647e8",
   "metadata": {},
   "source": [
    "#### Step 13. Create the connector for the paraphrase-multilingual-MiniLM-L12-v2 model\n",
    "\n",
    "Now we will create the connector for the paraphrase-multilingual-MiniLM-L12-v2 model that we created in step 8.  This tells OpenSearch where to send the request to get the embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173c3319-65b6-4774-9f37-615ee9d8c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register repository\n",
    "path = '/_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "  \"description\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"roleArn\": sageMakerOpenSearchRoleArn\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"service_name\": \"sagemaker\"\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"url\": \"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/\" + MiniLML12v2_predictor.endpoint_name + \"/invocations\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/json\"\n",
    "      },\n",
    "      \"request_body\": \"${parameters.input}\",\n",
    "      \"pre_process_function\": \"connector.pre_process.default.embedding\",\n",
    "      \"post_process_function\": \"connector.post_process.default.embedding\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "MiniLML12v2_connector_response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "MiniLML12v2_connector = MiniLML12v2_connector_response.json()[\"connector_id\"]\n",
    "print('Connector id: ' + MiniLML12v2_connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c018e6-295c-4941-854a-5806791a9da8",
   "metadata": {},
   "source": [
    "#### Step 14. Create the connector for the paraphrase-multilingual-MiniLM-L12-v2 model\n",
    "\n",
    "Now we will create the connector for the paraphrase-multilingual-MiniLM-L12-v2 model that we created in step 10.  This tells OpenSearch where to send the request to get the embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9687cc-541a-41e9-942e-83748e11a7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register repository\n",
    "path = '/_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "  \"description\": \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"roleArn\": sageMakerOpenSearchRoleArn\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"service_name\": \"sagemaker\"\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"url\": \"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/\" + mpnetbasev2_predictor.endpoint_name + \"/invocations\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/json\"\n",
    "      },\n",
    "      \"request_body\": \"${parameters.input}\",\n",
    "      \"pre_process_function\": \"connector.pre_process.default.embedding\",\n",
    "      \"post_process_function\": \"connector.post_process.default.embedding\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "mpnetbasev2_connector_response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "mpnetbasev2_connector = mpnetbasev2_connector_response.json()[\"connector_id\"]\n",
    "print('Connector id: ' + mpnetbasev2_connector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64808853-a3d6-4c8d-a6cb-79d5684c19d1",
   "metadata": {},
   "source": [
    "#### Step 14. Create the OpenSearch model group\n",
    "\n",
    "Next we create a model group to hold the models we are deploying to OpenSearch via SageMaker endpoints.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085a370-93d3-4574-9b3b-027ca26c5f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_plugins/_ml/model_groups/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"external_models\",\n",
    "  \"description\": \"A model group for external models\"\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print('Group id: ' + response.json()['model_group_id'])\n",
    "group_id = response.json()['model_group_id']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0623536-d4ba-4904-b50a-e4f7ebf981a2",
   "metadata": {},
   "source": [
    "#### Step 15. Register the paraphrase-multilingual-MiniLM-L12-v2 model\n",
    "\n",
    "We now register the paraphrase-multilingual-MiniLM-L12-v2 model to the model group and the connector that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73106637-fdae-4792-99c8-a7f9341e5174",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"model_group_id\": group_id,\n",
    "    \"description\": \"multilingual vector model\",\n",
    "    \"connector_id\": MiniLML12v2_connector\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "minilm_model_id = response.json()['model_id']\n",
    "print('Model id: ' + minilm_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b07c20a-f83f-49d4-9c92-31ba58e37c69",
   "metadata": {},
   "source": [
    "#### Step 16. Register the paraphrase-multilingual-mpnet-base-v2 model\n",
    "\n",
    "We now register the paraphrase-multilingual-mpnet-base-v2 model to the model group and the connector that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97532a46-0b38-4864-8b51-916ca3487252",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"paraphrase-multilingual-mpnet-base-v2\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"model_group_id\": group_id,\n",
    "    \"description\": \"multilingual vector model\",\n",
    "    \"connector_id\": mpnetbasev2_connector\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "mpnet_model_id = response.json()['model_id']\n",
    "print('Model id: ' + mpnet_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e82e41c-4e58-42f9-bc9c-646ea1150f14",
   "metadata": {},
   "source": [
    "#### Step 17. Deploy the paraphrase-multilingual-MiniLM-L12-v2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e290f440-d630-49da-8556-1a0ea47981e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ minilm_model_id + '/_deploy'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32491a4f-c7af-4268-911f-6f147c930af0",
   "metadata": {},
   "source": [
    "#### Step 18. Deploy the paraphrase-multilingual-mpnet-base-v2 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da8707-0e3f-403f-94f0-5de2ed402f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ mpnet_model_id + '/_deploy'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fff8403-4865-4349-8924-56b45fcb0018",
   "metadata": {},
   "source": [
    "#### Step 19. Test the paraphrase-multilingual-MiniLM-L12-v2 model through OpenSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1de584-cea9-4415-95bd-97a4739dedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ minilm_model_id + '/_predict'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"parameters\": {\n",
    "    \"input\": [\"It's nice to see the flowers bloom and hear the birds sing in the spring\"]\n",
    "  }\n",
    "}\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d18b1-9cac-4314-9b4b-cb306281ee88",
   "metadata": {},
   "source": [
    "#### Step 20. Deploy the paraphrase-multilingual-mpnet-base-v2 model through OpenSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c275112-ffde-4d0f-8425-89641481a9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ mpnet_model_id + '/_predict'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"parameters\": {\n",
    "    \"input\": [\"It's nice to see the flowers bloom and hear the birds sing in the spring\"]\n",
    "  }\n",
    "}\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411bceaa-c3b8-4765-ad66-5f912a504d93",
   "metadata": {},
   "source": [
    "#### Step 21. Create the paraphrase-multilingual-MiniLM-L12-v2 index pipeline\n",
    "\n",
    "Now we will create the pipeline for the index, this is how we tell OpenSearch to send the field(s) we wanted embeddings for to the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13c1580-e6d8-4b37-bf00-76d5b8340d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_ingest/pipeline/paraphrase-multilingual-MiniLM-L12-v2-pipeline'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"description\": \"Sagemaker paraphrase-multilingual-MiniLM-L12-v2 Pipeline\",\n",
    "  \"processors\":[\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": minilm_model_id,\n",
    "        \"field_map\": {\n",
    "           \"sentence\": \"sentence_vector\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c11fe8f-60a2-4cb5-815b-be0211ae5cb8",
   "metadata": {},
   "source": [
    "#### Step 22. Create the paraphrase-multilingual-mpnet-base-v2 index pipeline\n",
    "\n",
    "Now we will create the pipeline for the index, this is how we tell OpenSearch to send the field(s) we wanted embeddings for to the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761a6b4-69c3-4cb0-aee9-56edaf8ebc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_ingest/pipeline/paraphrase-multilingual-mpnet-base-v2-pipeline'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"description\": \"Sagemaker paraphrase-multilingual-mpnet-base-v2 Pipeline\",\n",
    "  \"processors\":[\n",
    "    {\n",
    "      \"text_embedding\": {\n",
    "        \"model_id\": mpnet_model_id,\n",
    "        \"field_map\": {\n",
    "           \"sentence\": \"sentence_vector\"\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c3dad-004a-4238-9c90-f55b22fbd33b",
   "metadata": {},
   "source": [
    "#### Step 23. Create the paraphrase-multilingual-MiniLM-L12-v2 index\n",
    "\n",
    "Next we create the index using the pipeline.  You can see we have three fields in the index:\n",
    "1. sentence_vector - this is where the vector embedding will be stored when returned from SageMaker\n",
    "2. sentence - this is the native language sentence\n",
    "3. sentence_english - this is the english translation of the sentence, it is just here as I don't speak all three languages to see how well the model is doing :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c8fa8-c8c3-4903-815c-d8b7728a94c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/paraphrase-multilingual-minilm-l12-v2-index'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"settings\": {\n",
    "    \"index.knn\": \"true\",\n",
    "    \"default_pipeline\": \"paraphrase-multilingual-MiniLM-L12-v2-pipeline\",\n",
    "    \"number_of_shards\": 4,\n",
    "    \"number_of_replicas\": 2\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"sentence_vector\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 384,\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"nmslib\",\n",
    "          \"space_type\": \"cosinesimil\"\n",
    "        }\n",
    "      },\n",
    "      \"sentence\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"sentence_english\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"id\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da82bd16-4f8d-4428-a308-f4e262a685b0",
   "metadata": {},
   "source": [
    "#### Step 24. Create the paraphrase-multilingual-mpnet-base-v2 index\n",
    "\n",
    "Next we create the index using the pipeline.  You can see we have three fields in the index:\n",
    "1. sentence_vector - this is where the vector embedding will be stored when returned from SageMaker\n",
    "2. sentence - this is the native language sentence\n",
    "3. sentence_english - this is the english translation of the sentence, it is just here as I don't speak all three languages to see how well the model is doing :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd504081-22de-4dd6-b386-28ea29e6c892",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/paraphrase-multilingual-mpnet-base-v2-index'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"settings\": {\n",
    "    \"index.knn\": \"true\",\n",
    "    \"default_pipeline\": \"paraphrase-multilingual-mpnet-base-v2-pipeline\",\n",
    "    \"number_of_shards\": 4,\n",
    "    \"number_of_replicas\": 2\n",
    "  },\n",
    "  \"mappings\": {\n",
    "    \"properties\": {\n",
    "      \"sentence_vector\": {\n",
    "        \"type\": \"knn_vector\",\n",
    "        \"dimension\": 768,\n",
    "        \"method\": {\n",
    "          \"name\": \"hnsw\",\n",
    "          \"engine\": \"nmslib\",\n",
    "          \"space_type\": \"cosinesimil\"\n",
    "        }\n",
    "      },\n",
    "      \"sentence\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"sentence_english\": {\n",
    "        \"type\": \"text\"\n",
    "      },\n",
    "      \"id\": {\n",
    "        \"type\": \"text\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abcf080-711d-486e-80c3-0fd4dc503b23",
   "metadata": {},
   "source": [
    "#### Step 25. Send the sentences to the paraphrase-multilingual-minilm-l12-v2-index OpenSearch index for indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00eb346b-05b0-43f5-a29e-5cd3da53f8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "hostname = host[8:len(host)] \n",
    "region = 'us-east-1'\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': hostname, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")\n",
    "\n",
    "with open(\"english.json\", 'r') as file:\n",
    "    english = file.read()\n",
    "\n",
    "with open(\"french.json\", 'r') as file:\n",
    "    french = file.read()\n",
    "\n",
    "with open(\"german.json\", 'r') as file:\n",
    "    german = file.read()\n",
    "docs = english + french + german\n",
    "\n",
    "results = client.bulk(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60318d5-c430-46e9-9d39-e3864536e08e",
   "metadata": {},
   "source": [
    "#### Step 26. Send the sentences to the paraphrase-multilingual-mpnet-base-v2-index OpenSearch index for indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475bc63b-a640-49aa-bf3f-a3ef429853b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "auth = AWSV4SignerAuth(credentials, region, service)\n",
    "\n",
    "client = OpenSearch(\n",
    "    hosts = [{'host': hostname, 'port': 443}],\n",
    "    http_auth = auth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection,\n",
    "    pool_maxsize = 20\n",
    ")\n",
    "\n",
    "with open(\"english.json\", 'r') as file:\n",
    "    english = file.read()\n",
    "\n",
    "with open(\"french.json\", 'r') as file:\n",
    "    french = file.read()\n",
    "\n",
    "with open(\"german.json\", 'r') as file:\n",
    "    german = file.read()\n",
    "docs = english + french + german\n",
    "\n",
    "docs = docs.replace(\"paraphrase-multilingual-minilm-l12-v2-index\", \"paraphrase-multilingual-mpnet-base-v2-index\")\n",
    "\n",
    "results = client.bulk(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31af191c",
   "metadata": {},
   "source": [
    "#### Step 27. Test the indexes\n",
    "\n",
    "From the OpenSearch dashboard, click on the hamburger menu at the top left and select **Search Relevance**\n",
    "\n",
    "![Search Releveance screen](images/search_relevance.png)\n",
    "\n",
    "On the Search relevance screen select **paraphrase-multilingual-minilm-l2-v2-index** for the Query 1 **index** and the following code in the Query 1 **Query** text box.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"sentence_vector\": {\n",
    "        \"query_text\": \"%SearchText%\",\n",
    "        \"model_id\": \"<model_id from step 21>\",\n",
    "        \"k\": 30\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"size\": \"30\",\n",
    "  \"_source\": [\"sentence\", \"sentence_english\"]\n",
    "}\n",
    "```\n",
    "\n",
    "select **paraphrase-multilingual-mpneet-base-v2-index** for the Query 2 **index** and the following code in the Query 2 **Query** text box.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"neural\": {\n",
    "      \"sentence_vector\": {\n",
    "        \"query_text\": \"%SearchText%\",\n",
    "        \"model_id\": \"<model_id from step 22>\",\n",
    "        \"k\": 30\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"size\": \"30\",\n",
    "  \"_source\": [\"sentence\", \"sentence_english\"]\n",
    "}\n",
    "```\n",
    "\n",
    "Some sample query terms are:\n",
    "- mechanical parts\n",
    "- the season after winter\n",
    "- moving quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee054c32-ca65-4e86-9de3-e1f6b894243f",
   "metadata": {},
   "source": [
    "#### Step 28. Cleanup resources\n",
    "\n",
    "We will now delete the following:\n",
    "1. OpenSearch Cluster\n",
    "2. paraphrase-multilingual-MiniLM-L12-v2 endpoint\n",
    "3. paraphrase-multilingual-mpnet-base-v2 endpoint\n",
    "4. Models stored in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c2afe9-4a0d-44b7-823b-267adbbf42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "openSearchClient.delete_domain(\n",
    "    DomainName='multilingual-demo'\n",
    ")\n",
    "\n",
    "MiniLML12v2_predictor.delete_model()\n",
    "MiniLML12v2_predictor.delete_endpoint()\n",
    "\n",
    "mpnetbasev2_predictor.delete_model()\n",
    "mpnetbasev2_predictor.delete_endpoint()\n",
    "\n",
    "s3Client.delete_object(Bucket=s3BucketName, Key='custom_inference/' + path + '/model.tar.gz')\n",
    "s3Client.delete_object(Bucket=s3BucketName, Key='custom_inference/' + mpnetpath + '/model.tar.gz')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
