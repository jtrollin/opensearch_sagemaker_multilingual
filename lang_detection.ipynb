{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200d231b",
   "metadata": {},
   "source": [
    "# Using AWS Opensearch ML Commons REST API for language detection\n",
    "\n",
    "Amazon Opensearch 2.15.0 has a new ML inference processor that enables users to enrich ingest pipelines using inferences from OpenSearch-provided pretrained models. The ml_inference processor is used to invoke machine learning (ML) models registered in the OpenSearch ML Commons plugin. The model outputs are added as new fields to the ingested documents.\n",
    "\n",
    "ML Commons for OpenSearch makes it easy to develop new machine learning features within Opensearch. The plugin allows machine learning engineers and developers to leverage existing opensource machine learning algorithms and streamlines the efforts to build new machine learning features.  \n",
    "\n",
    "We will be deploying three models in this notebook, the first is an Amazon Comprehend model. The model examines the input text, detects the language using the Amazon Comprehend DetectDominantLanguage API, and sets a corresponding language code.\n",
    "\n",
    "The second model uses Amazon Sagemaker's built-in BlazingText algorithm—a highly optimized implementation of the Word2vec and text classification algorithms that scale to large datasets easily. It is useful for many downstream natural language processing (NLP) tasks. This notebook walks through creating an Amazon OpenSearch connector, model, ingest pipeline, and testing for both the Amazon Comprehend model and the BlazingText fasttext model.\n",
    "\n",
    "The third model is Amazon's  Titan embeddings model v2. In this notebook, we will use this embeddings model to create vectors of text in three languages (English, French and German). These vectors will then be stored in Amazon OpenSearch and allow for semantic searches to be used across the language sets.\n",
    "\n",
    "Note: This functionality is available in **Amazon OpenSearch** 2.15.0 or later (we release odd versions), and Opensearch 2.14.0 or later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6104781",
   "metadata": {},
   "source": [
    "#### Step 1. Install dependencies needed for this notebook.\n",
    "\n",
    "Ignore the ERROR about pip's dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd18d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker requests-aws4auth GitPython opensearch-py --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a4a1a",
   "metadata": {},
   "source": [
    "#### Step 2. Install git-lfs so that we can clone the model repos to our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da73a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install -y amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y \n",
    "!sudo yum-config-manager --enable epel\n",
    "!sudo yum install git-lfs -y\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ffc63",
   "metadata": {},
   "source": [
    "#### Step 3.  Store the Cloudformation output values as variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6dc860",
   "metadata": {},
   "source": [
    "This code block will store your cloudformation stack outputs as variables: **S3BucketName**, **SageMakerExecutionRoleArn**, **SageMakerOpenSearchRoleArn**, **OpensearchDashboardsURL**, **OpensearchDomainEndpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370439a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "cf_client = boto3.client('cloudformation')\n",
    "\n",
    "# Replace 'YourStackName' with the actual name of your CloudFormation stack\n",
    "stack_name = 'opensearch-lang-detect-demo'\n",
    "\n",
    "# Get stack outputs\n",
    "response = cf_client.describe_stacks(StackName=stack_name)\n",
    "outputs = response['Stacks'][0]['Outputs']\n",
    "\n",
    "# Create a dictionary to store the outputs\n",
    "output_dict = {output['OutputKey']: output['OutputValue'] for output in outputs}\n",
    "\n",
    "# Retrieve values from the output dictionary\n",
    "s3BucketName = output_dict.get('S3BucketName', 'Not found')\n",
    "sageMakerExecutionRoleArn = output_dict.get('SageMakerExecutionRoleArn', 'Not found')\n",
    "sageMakerOpenSearchRoleArn = output_dict.get('SageMakerOpenSearchRoleArn', 'Not found')\n",
    "opensearchDomainEndpoint = output_dict.get('OpenSearchDomainEndpoint', 'Not found')\n",
    "\n",
    "print('S3 Bucket Name: ' + s3BucketName)\n",
    "print('SageMaker Execution Role Arn: ' + sageMakerExecutionRoleArn)\n",
    "print('SageMaker OpenSearch Role Arn: ' + sageMakerOpenSearchRoleArn)\n",
    "print('OpenSearch Domain Endpoint: ' + opensearchDomainEndpoint)\n",
    "\n",
    "if 'OpenSearchDashboardsURL' in output_dict:\n",
    "    print('OpenSearch Dashboards URL: ' + output_dict['OpenSearchDashboardsURL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a341e8e5",
   "metadata": {},
   "source": [
    "#### Step 5. Add the SageMaker Execution role to OpenSearch\n",
    "\n",
    "For us to be able to interact with OpenSearch from the notebook we need to allow the SageMaker execution role that was created by the CloudFormationTemplate to perform actions in OpenSearch.\n",
    "\n",
    "Navigate to OpenSearch Console page in a different tab within your AWS Account. Click on the **ml-commons-demo-2-17** domain. Click on the OpenSearch Dashboards URL (IPv4) and login using the username and password you provided when deploying your cloudformation stack.  \n",
    "![Dashboard](images/1_dashboard.png)\n",
    "\n",
    "Then navigate to Security using the left hand menu.\n",
    "![Security](images/2_security.png)\n",
    "\n",
    "Next select **Roles** from the Security left hand menu.\n",
    "![Roles](images/3_roles.png)\n",
    "\n",
    "From the roles screen select **all_access**\n",
    "![all access](images/4_all_access.png)\n",
    "\n",
    "Select the **Mapped users** tab and then click on the **Manage mapping** button.\n",
    "![mapped users](images/5_mapped_users.png)\n",
    "\n",
    "Provide the **SageMaker Execution Role Arn**. (this was printed out in Step 3 above)\n",
    "![mapped users](images/6_backend_roles.png)\n",
    "\n",
    "Click on the **Map** button.\n",
    "\n",
    "Navigate back to the **Roles** screen by using the breadcrumb at the top of the dashboard.\n",
    "\n",
    "Search for the **ml_full_access** role and select it.\n",
    "![mapped users](images/7_ml_full_access.png)\n",
    "\n",
    "Select the **Mapped users** tab and then click on the **Manage mapping** button.\n",
    "![mapped users](images/8_ml_full_access_tabs.png)\n",
    "\n",
    "Provide the **SageMaker OpenSearch Role Arn** (this was printed out in Step 3 above)\n",
    "![mapped users](images/9_add_role.png)\n",
    "\n",
    "Click on the **Map** button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341b9b3",
   "metadata": {},
   "source": [
    "#### Step 6. Setup the commons connector\n",
    "\n",
    "We need to enable access control for the connector to talk to SageMaker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import boto3\n",
    "import time\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "service = 'es'\n",
    "session = boto3.Session()\n",
    "region = session.region_name\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, service, session_token=credentials.token)\n",
    "\n",
    "host= 'https://' + opensearchDomainEndpoint\n",
    "\n",
    "# Register repository\n",
    "path = '/_cluster/settings'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"persistent\": {\n",
    "        \"plugins.ml_commons.trusted_connector_endpoints_regex\": [\n",
    "        \"\"\"^https://runtime\\.sagemaker\\..*[a-z0-9-]\\.amazonaws\\.com/.*$\"\"\",\n",
    "        \"\"\"^https://api\\.openai\\.com/.*$\"\"\",\n",
    "        \"\"\"^https://api\\.cohere\\.ai/.*$\"\"\",\n",
    "        \"\"\"^https://bedrock-runtime\\..*[a-z0-9-]\\.amazonaws\\.com/.*$\"\"\",\n",
    "        \"\"\"^https://comprehend\\..*[a-z0-9-]\\.amazonaws\\.com$\"\"\",\n",
    "        \"\"\"^https://textract\\..*[a-z0-9-]\\.amazonaws\\.com$\"\"\",\n",
    "        \"\"\"^https://translate\\..*[a-z0-9-]\\.amazonaws\\.com$\"\"\",\n",
    "        \"\"\"^https://rekognition\\..*[a-z0-9-]\\.amazonaws\\.com$\"\"\",\n",
    "        \"\"\"^https://personalize\\..*[a-z0-9-]\\.amazonaws\\.com.*$\"\"\",\n",
    "        \"\"\"^https://personalize-runtime\\..*[a-z0-9-]\\.amazonaws\\.com.*$\"\"\"\n",
    "    ]\n",
    "    }\n",
    "    }\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef59b0b",
   "metadata": {},
   "source": [
    "## Comprehend Language Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f85310",
   "metadata": {},
   "source": [
    "The first model will be built from the Amazon Comprehend service. This service examines the input text, detects the language using the Amazon Comprehend DetectDominantLanguage API, and sets a corresponding language code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e6a7f",
   "metadata": {},
   "source": [
    "#### Step 7. Create the connector for the Comprehend Language Classification model\n",
    "\n",
    "Now we will create the connector and model for Amazon Comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client('comprehend', region_name='us-east-1')\n",
    "path = '/_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"Comprehend lang identification\",\n",
    "  \"description\": \"comprehend model\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"roleArn\": sageMakerOpenSearchRoleArn\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"service_name\": \"comprehend\",\n",
    "    \"api_version\": \"20171127\",\n",
    "    \"api_name\": \"DetectDominantLanguage\",\n",
    "    \"api\": \"Comprehend_${parameters.api_version}.${parameters.api_name}\",\n",
    "    \"response_filter\": \"$\"\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"url\": \"https://${parameters.service_name}.${parameters.region}.amazonaws.com\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/x-amz-json-1.1\",\n",
    "        \"X-Amz-Target\": \"${parameters.api}\"\n",
    "      },\n",
    "      \"request_body\": \"{\\\"Text\\\": \\\"${parameters.Text}\\\"}\" \n",
    "    }\n",
    "  ]\n",
    "}\n",
    "# headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "comprehend_connector_response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "comprehend_connector = comprehend_connector_response.json()[\"connector_id\"]\n",
    "print('Connector id: ' + comprehend_connector)\n",
    "# print(comprehend_connector_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c490574",
   "metadata": {},
   "source": [
    "#### Step 8. Register the Comprehend model\n",
    "\n",
    "We now register the Comprehend model to the model group and the connector that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a191115",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"comprehend lang id model\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"test model\",\n",
    "    \"connector_id\": comprehend_connector\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "# print(response.json())\n",
    "comprehend_model_id = response.json()['model_id']\n",
    "print('Model id: ' + comprehend_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9b132",
   "metadata": {},
   "source": [
    "#### Step 9. Deploy the Comprehend model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ comprehend_model_id + '/_deploy'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b7d6c",
   "metadata": {},
   "source": [
    "#### Step 10. Test the Comprehend model through OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0fe342",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ comprehend_model_id + '/_predict'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"parameters\": {\n",
    "        \"Text\": \"你知道厕所在哪里吗\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca70cd",
   "metadata": {},
   "source": [
    "#### Step 11. Create the comprehend index pipeline\n",
    "\n",
    "Now we will create the pipeline for the index, this is how we tell OpenSearch to send the field(s) we wanted translations for to the Comprehend endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_ingest/pipeline/comprehend_language_identification_pipeline'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"description\": \"ingest reviews and identify lang with the comprehend model\",\n",
    "  \"processors\":[\n",
    "    {\n",
    "      \"ml_inference\": {\n",
    "        \"model_id\": comprehend_model_id,\n",
    "        \"input_map\": [\n",
    "            {\n",
    "               \"Text\": \"Text\"\n",
    "            }\n",
    "        ],\n",
    "        \"output_map\": [\n",
    "            {\n",
    "                \n",
    "            \"detected_language\": \"response.Languages[0].LanguageCode\",\n",
    "            \"language_score\": \"response.Languages[0].Score\"\n",
    "            }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153a9e5",
   "metadata": {},
   "source": [
    "#### Step 12. Create the Comprehend index & test\n",
    "\n",
    "Next we create the index using the pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "index_name = 'comprehend_lang_ident_test01'\n",
    "path = '/' + index_name\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"default_pipeline\": \"comprehend_language_identification_pipeline\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the PUT request to create the index\n",
    "url = host + path\n",
    "response = requests.put(url, auth=awsauth, json=index_settings, headers=headers)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'comprehend_lang_ident_test01'\n",
    "path = '/' + index_name + '/_doc/'\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Define the document to index\n",
    "document = {\n",
    "    \"Text\": \"parlez vous francais?\"\n",
    "}\n",
    "\n",
    "url = host + path\n",
    "response = requests.post(url, auth=awsauth, json=document, headers=headers)\n",
    "\n",
    "\n",
    "print(response.json())\n",
    "\n",
    "doc_id = response.json()['_id']\n",
    "\n",
    "# Retrieve the indexed document\n",
    "get_path = '/' + index_name + '/_doc/' + doc_id\n",
    "get_url = host + get_path\n",
    "get_response = requests.get(get_url, auth=awsauth, headers=headers)\n",
    "\n",
    "# Print the retrieved document\n",
    "print(get_response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd32ae",
   "metadata": {},
   "source": [
    "## Fasttext Language Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8761f",
   "metadata": {},
   "source": [
    "Now that we've tested our Amazon Comprehend model connector, let's create a connector for our Sagemaker model using FastText language identification supported by BlazingText\n",
    "\n",
    "More information about this algorithm can be found here:https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html\n",
    "\n",
    "BlazingText is a GPU-accelerated implementation of FastText, capable of hosting pre-trained Text Classification and Word2Vec models, including FastText models. FastText is a neural network model used for both unsupervised word embedding generation and supervised text classification.\n",
    "\n",
    "While BlazingText employs custom CUDA kernels to speed up FastText's training process, the core algorithm remains the same for both. This compatibility allows users to leverage BlazingText's hosting capabilities on Amazon SageMaker for real-time predictions using FastText models. This is particularly useful if you have your own FastText-trained model or if one of the pre-trained models provided by the FastText team meets your requirements.\n",
    "\n",
    "In essence, BlazingText offers a way to deploy FastText models on SageMaker endpoints, combining the benefits of FastText's versatility with the computational efficiency of GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8247dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "import os.path\n",
    "import tarfile\n",
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"blazingtext\", region=region_name)\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))\n",
    "\n",
    "!wget -O model.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
    "# This creates a tar file fromt he model and uploads it into our S3 bucket we created in Step 3. to store models\n",
    "!tar -czvf langid.tar.gz model.bin\n",
    "model_location = sess.upload_data(\"langid.tar.gz\", bucket=s3BucketName, key_prefix='custom_inference/fasttext-language-identification')\n",
    "!rm langid.tar.gz model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7b460",
   "metadata": {},
   "source": [
    "#### Step 1. Deploy the fasttext model\n",
    "\n",
    "deploy the model as a SageMaker endpoint.\n",
    "\n",
    "*note: takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703181e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "from sagemaker.predictor import Predictor\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = sageMakerExecutionRoleArn\n",
    "    \n",
    "model_location = 's3://' + s3BucketName + '/custom_inference/fasttext-language-identification/langid.tar.gz'\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\"blazingtext\", region=region_name)\n",
    "\n",
    "\n",
    "lang_id = sagemaker.Model(model_data=model_location, image_uri=container, role=role, sagemaker_session=sess)\n",
    "endpoint_name = 'fasttext3'\n",
    "lang_id.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge',endpoint_name=endpoint_name)\n",
    "\n",
    "FT_predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sess)\n",
    "FT_predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "FT_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e14335",
   "metadata": {},
   "source": [
    "#### Step 2. Test the fasttext endpoint\n",
    "\n",
    "Now we will test the newly created endpoint to see it gives us accurate language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b131ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"hi which language is this?\",\n",
    "             \"mon nom est Pierre\",\n",
    "             \"Dem Jungen gab ich einen Ball.\",\n",
    "             \"আমি বাড়ি যাবো.\"]\n",
    "payload = {\"instances\" : sentences}\n",
    "\n",
    "predictions = FT_predictor.predict(payload)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb246c0",
   "metadata": {},
   "source": [
    "#### Step 3. Create the connector for the fasttext model\n",
    "\n",
    "Now we will create the connector and model for the fasttext model through Amazon Opensearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb71335",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"lang identification\",\n",
    "  \"description\": \"fasttext model\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"roleArn\": sageMakerOpenSearchRoleArn\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"service_name\": \"sagemaker\"\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"url\": \"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/\" + FT_predictor.endpoint_name + \"/invocations\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/json\"\n",
    "      },\n",
    "      \"request_body\": \"{ \\\"instances\\\": [ \\\"${parameters.text}\\\" ] }\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "ft_connector_response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "ft_connector = ft_connector_response.json()[\"connector_id\"]\n",
    "print('Connector id: ' + ft_connector)\n",
    "# print(comprehend_connector_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb64af1",
   "metadata": {},
   "source": [
    "#### Step 4. Register the fasttext model\n",
    "\n",
    "We now register the fasttext model to the model group and the connector that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"fasttext\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"lang id model\",\n",
    "    \"connector_id\": ft_connector\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "ft_model_id = response.json()['model_id']\n",
    "print('Model id: ' + ft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9982d4",
   "metadata": {},
   "source": [
    "#### Step 5. Deploy the fasttext model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ ft_model_id + '/_deploy'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b54bdb",
   "metadata": {},
   "source": [
    "#### Step 6. Test the fasttext model through OpenSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a253808",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ ft_model_id + '/_predict'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"parameters\": {\n",
    "    \"text\": \"It's nice to see the flowers bloom and hear the birds sing in the spring\"\n",
    "  }\n",
    "}\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6f5e9",
   "metadata": {},
   "source": [
    "#### Step 7 Create the fasttext index pipeline\n",
    "\n",
    "Now we will create the pipeline for the index, this is how we tell OpenSearch to send the field(s) we wanted translations for to the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc03a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_ingest/pipeline/ft_language_identification_pipeline'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"description\": \"ingest reviews and identify lang with the fasttext model via sagemaker endpoint\",\n",
    "  \"processors\":[\n",
    "    {\n",
    "      \"ml_inference\": {\n",
    "        \"model_id\": ft_model_id,\n",
    "        \"input_map\": [\n",
    "            {\n",
    "               \"text\": \"text\"\n",
    "            }\n",
    "        ],\n",
    "        \"output_map\": [\n",
    "            {\n",
    "                \"inference\":\"response[0].label\"\n",
    "            }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d77e02",
   "metadata": {},
   "source": [
    "#### Step 8. Create the fasttext index & test\n",
    "\n",
    "Next we create the index using the pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1faf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "index_name = 'ft_lang_ident_test1'\n",
    "path = '/' + index_name\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"default_pipeline\": \"ft_language_identification_pipeline\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the PUT request to create the index\n",
    "url = host + path\n",
    "response = requests.put(url, auth=awsauth, json=index_settings, headers=headers)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf28c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_name = 'ft_lang_ident_test1'\n",
    "path = '/' + index_name + '/_doc/'\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Define the document to index\n",
    "document = {\n",
    "    \"text\": \"parlez vous francais?\"\n",
    "}\n",
    "\n",
    "url = host + path\n",
    "response = requests.post(url, auth=awsauth, json=document, headers=headers)\n",
    "\n",
    "\n",
    "print(response.json())\n",
    "\n",
    "doc_id = response.json()['_id']\n",
    "\n",
    "# Retrieve the indexed document\n",
    "get_path = '/' + index_name + '/_doc/' + doc_id\n",
    "get_url = host + get_path\n",
    "get_response = requests.get(get_url, auth=awsauth, headers=headers)\n",
    "\n",
    "# Print the retrieved document\n",
    "print(get_response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c964474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
