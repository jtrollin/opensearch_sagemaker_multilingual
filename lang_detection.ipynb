{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "200d231b",
   "metadata": {},
   "source": [
    "# Using AWS Opensearch ML Commons REST API for language detection\n",
    "\n",
    "Amazon Opensearch 2.15.0 has a new ML inference processor that enables users to enrich ingest pipelines using inferences from OpenSearch-provided pretrained models. The ml_inference processor is used to invoke machine learning (ML) models registered in the OpenSearch ML Commons plugin. The model outputs are added as new fields to the ingested documents.\n",
    "\n",
    "ML Commons for OpenSearch makes it easy to develop new machine learning features within Opensearch. The plugin allows machine learning engineers and developers to leverage existing opensource machine learning algorithms and streamlines the efforts to build new machine learning features.  \n",
    "\n",
    "We will be deploying two models in this notebook, the first is an Amazon Comprehend model. The model examines the input text, detects the language using the Amazon Comprehend DetectDominantLanguage API, and sets a corresponding language code.\n",
    "\n",
    "The second model uses Amazon Sagemaker's built-in BlazingText algorithm—a highly optimized implementation of the Word2vec and text classification algorithms that scale to large datasets easily. It is useful for many downstream natural language processing (NLP) tasks.\n",
    "\n",
    "This notebook walks through creating an Amazon OpenSearch connector, model, ingest pipeline, and testing for both the Amazon Comprehend model and the BlazingText fasttext model.\n",
    "\n",
    "Note: This functionality is available in **Amazon OpenSearch** 2.15.0 or later (we release odd versions), and Opensearch 2.14.0 or later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6104781",
   "metadata": {},
   "source": [
    "#### Step 1. Install dependancies needed for this notebook.\n",
    "\n",
    "Ignore the ERROR about pip's dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd18d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker requests-aws4auth GitPython opensearch-py --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a4a1a",
   "metadata": {},
   "source": [
    "#### Step 2. Install git-lfs so that we can clone the model repos to our notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da73a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo yum install -y amazon-linux-extras\n",
    "!sudo amazon-linux-extras install epel -y \n",
    "!sudo yum-config-manager --enable epel\n",
    "!sudo yum install git-lfs -y\n",
    "!git lfs install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461ffc63",
   "metadata": {},
   "source": [
    "#### Step 3.  Store the ARNs of the roles that you created as variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370439a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3BucketName = 'insert bucket name'\n",
    "sageMakerExecutionRoleArn = 'insert SageMaker-Execution-demo-role ARN here'\n",
    "sageMakerOpenSearchRoleArn = 'insert SageMaker-OpenSearch-demo-role ARN here'\n",
    "\n",
    "print('S3 Bucket Name: ' + s3BucketName)\n",
    "print('SageMaker Execution Role Arn: ' + sageMakerExecutionRoleArn)\n",
    "print('SageMaker OpenSearch Role Arn: ' + sageMakerOpenSearchRoleArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fe62e4",
   "metadata": {},
   "source": [
    "#### Step 4. Create the opensearch cluster\n",
    "\n",
    "This will create an OpenSearch cluster (2.15.0) for use doing the demo.\n",
    "\n",
    "Please update the code below, you will need to provide your own `username` and `password` on lines 5 and 6 below before running the code block.\n",
    "\n",
    "*note: this will take several minutes (up to 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bfd483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "username= \"insert username here\"\n",
    "password=\"insert password here\"\n",
    "\n",
    "openSearchClient = boto3.client('opensearch')\n",
    "stsClient = boto3.client('sts')\n",
    "# openSearchClient = boto3.client('opensearch', region_name='us-west-2')\n",
    "# stsClient = boto3.client('sts', region_name='us-west-2')\n",
    "service = 'aoss'\n",
    "region = 'us-east-1'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, service, session_token=credentials.token)\n",
    "                   \n",
    "AWS_ACCOUNT_ID = stsClient.get_caller_identity()[\"Account\"]\n",
    "\n",
    "domainName = 'ml-commons-demo-2-17'\n",
    "\n",
    "createResponse = openSearchClient.create_domain(\n",
    "    DomainName=domainName,\n",
    "    EngineVersion='OpenSearch_2.17',\n",
    "    ClusterConfig={\n",
    "        'InstanceType': 'r5.large.search',\n",
    "        'InstanceCount': 3,\n",
    "        'DedicatedMasterEnabled': True,\n",
    "        'DedicatedMasterType': 'r5.large.search',\n",
    "        'DedicatedMasterCount': 3,\n",
    "    },\n",
    "    EBSOptions={\n",
    "        'EBSEnabled': True,\n",
    "        'VolumeType': 'gp3',\n",
    "        'VolumeSize': 100,\n",
    "        'Iops': 3500,\n",
    "        'Throughput': 125\n",
    "    },\n",
    "    AccessPolicies=f'{{\\\"Version\\\":\\\"2012-10-17\\\",\\\"Statement\\\":[{{\\\"Effect\\\":\\\"Allow\\\",\\\"Principal\\\":{{\\\"AWS\\\":\\\"*\"}},\\\"Action\\\":\\\"es:*\\\",\\\"Resource\\\":\\\"arn:aws:es:us-east-1:{AWS_ACCOUNT_ID}:domain\\/{domainName}\\/*\\\"}}]}}',\n",
    "    IPAddressType='ipv4',\n",
    "    NodeToNodeEncryptionOptions={\n",
    "        'Enabled': True\n",
    "    },\n",
    "    DomainEndpointOptions={\n",
    "        'EnforceHTTPS': True,\n",
    "        'TLSSecurityPolicy': 'Policy-Min-TLS-1-2-PFS-2023-10',\n",
    "    },\n",
    "    AdvancedSecurityOptions={\n",
    "        'Enabled': True,\n",
    "        'InternalUserDatabaseEnabled': True,\n",
    "        'MasterUserOptions': {\n",
    "            'MasterUserName': username,\n",
    "            'MasterUserPassword': password,\n",
    "        },\n",
    "    },\n",
    "    EncryptionAtRestOptions={\n",
    "        'Enabled': True\n",
    "    }\n",
    ")\n",
    "\n",
    "domainState = 'Processing'\n",
    "while domainState != 'Active':\n",
    "    time.sleep(10)\n",
    "    status = openSearchClient.describe_domain_health(\n",
    "        DomainName=domainName\n",
    "    )\n",
    "    domainState = status['DomainState']\n",
    "\n",
    "domaininfo = openSearchClient.describe_domain(\n",
    "    DomainName=domainName\n",
    ")\n",
    "while True:\n",
    "    if 'Endpoint' in domaininfo['DomainStatus'].keys():\n",
    "        break\n",
    "    else:\n",
    "        time.sleep(10)\n",
    "        domaininfo = openSearchClient.describe_domain(\n",
    "            DomainName=domainName\n",
    "        )\n",
    "\n",
    "host = 'https://' + domaininfo['DomainStatus']['Endpoint']\n",
    "print('Cluster URL: ' + host)\n",
    "print('Dashboard URL: ' + host + '/_dashboards')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a341e8e5",
   "metadata": {},
   "source": [
    "#### Step 5. Add the SageMaker Execution role to OpenSearch\n",
    "\n",
    "For us to be able to interact with OpenSearch from the notebook we need to allow the SageMaker execution role that was created by the CloudFormationTemplate to perform actions in OpenSearch.\n",
    "\n",
    "Navigate to OpenSearch Dashboard (from the Dashboard URL created in step 4) and login using the username and password you provided above.  \n",
    "![Dashboard](images/1_dashboard.png)\n",
    "\n",
    "Then navigate to Security using the left hand menu.\n",
    "![Security](images/2_security.png)\n",
    "\n",
    "Next select **Roles** from the Security left hand menu.\n",
    "![Roles](images/3_roles.png)\n",
    "\n",
    "From the roles screen select **all_access**\n",
    "![all access](images/4_all_access.png)\n",
    "\n",
    "Select the **Mapped users** tab and then click on the **Manage mapping** button.\n",
    "![mapped users](images/5_mapped_users.png)\n",
    "\n",
    "Provide the **SageMaker Execution Role Arn**. (this was printed out in Step 3 above)\n",
    "![mapped users](images/6_backend_roles.png)\n",
    "\n",
    "Click on the **Map** button.\n",
    "\n",
    "Navigate back to the **Roles** screen by using the breadcrumb at the top of the dashboard.\n",
    "\n",
    "Search for the **ml_full_access** role and select it.\n",
    "![mapped users](images/7_ml_full_access.png)\n",
    "\n",
    "Select the **Mapped users** tab and then click on the **Manage mapping** button.\n",
    "![mapped users](images/8_ml_full_access_tabs.png)\n",
    "\n",
    "Provide the **SageMaker OpenSearch Role Arn** (this was printed out in Step 3 above)\n",
    "![mapped users](images/9_add_role.png)\n",
    "\n",
    "Click on the **Map** button."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7341b9b3",
   "metadata": {},
   "source": [
    "#### Step 6. Setup the commons connector\n",
    "\n",
    "We need to enable access control for the connector to talk to SageMaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1523843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "# Register repository\n",
    "path = '/_cluster/settings'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"persistent\": {\n",
    "        \"plugins.ml_commons.trusted_connector_endpoints_regex\": [\n",
    "        \"\"\"^https://runtime\\.sagemaker\\..*[a-z0-9-]\\.amazonaws\\.com/.*$\"\"\",\n",
    "        \"\"\"^https://api\\.openai\\.com/.*$\"\"\",\n",
    "        \"\"\"^https://api\\.cohere\\.ai/.*$\"\"\",\n",
    "        \"\"\"^https://bedrock-runtime\\..*[a-z0-9-]\\.amazonaws\\.com/.*$\"\"\",\n",
    "        \"\"\"^https://comprehend\\..*[a-z0-9-]\\.amazonaws\\.com$\"\"\",\n",
    "        \"\"\"^https://textract\\..*[a-z0-9-]\\.amazonaws\\.com$\"\"\",\n",
    "        \"\"\"^https://translate\\..*[a-z0-9-]\\.amazonaws\\.com$\"\"\",\n",
    "        \"\"\"^https://rekognition\\..*[a-z0-9-]\\.amazonaws\\.com$\"\"\",\n",
    "        \"\"\"^https://personalize\\..*[a-z0-9-]\\.amazonaws\\.com.*$\"\"\",\n",
    "        \"\"\"^https://personalize-runtime\\..*[a-z0-9-]\\.amazonaws\\.com.*$\"\"\"\n",
    "    ]\n",
    "    }\n",
    "    }\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef59b0b",
   "metadata": {},
   "source": [
    "## Comprehend Language Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f85310",
   "metadata": {},
   "source": [
    "The first model will be built from the Amazon Comprehend service. This service examines the input text, detects the language using the Amazon Comprehend DetectDominantLanguage API, and sets a corresponding language code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777e6a7f",
   "metadata": {},
   "source": [
    "#### Step 7. Create the connector for the Comprehend Language Classification model\n",
    "\n",
    "Now we will create the connector and model for Amazon Comprehend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaa4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend = boto3.client('comprehend', region_name='us-east-1')\n",
    "path = '/_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"Comprehend lang identification\",\n",
    "  \"description\": \"comprehend model\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"roleArn\": sageMakerOpenSearchRoleArn\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"service_name\": \"comprehend\",\n",
    "    \"api_version\": \"20171127\",\n",
    "    \"api_name\": \"DetectDominantLanguage\",\n",
    "    \"api\": \"Comprehend_${parameters.api_version}.${parameters.api_name}\",\n",
    "    \"response_filter\": \"$\"\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"url\": \"https://${parameters.service_name}.${parameters.region}.amazonaws.com\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/x-amz-json-1.1\",\n",
    "        \"X-Amz-Target\": \"${parameters.api}\"\n",
    "      },\n",
    "      \"request_body\": \"{\\\"Text\\\": \\\"${parameters.Text}\\\"}\" \n",
    "    }\n",
    "  ]\n",
    "}\n",
    "# headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "comprehend_connector_response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "comprehend_connector = comprehend_connector_response.json()[\"connector_id\"]\n",
    "print('Connector id: ' + comprehend_connector)\n",
    "# print(comprehend_connector_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c490574",
   "metadata": {},
   "source": [
    "#### Step 9. Register the Comprehend model\n",
    "\n",
    "We now register the Comprehend model to the model group and the connector that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a191115",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"comprehend lang id model\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"test model\",\n",
    "    \"connector_id\": comprehend_connector\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "# print(response.json())\n",
    "comprehend_model_id = response.json()['model_id']\n",
    "print('Model id: ' + comprehend_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a9b132",
   "metadata": {},
   "source": [
    "#### Step 10. Deploy the Comprehend model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f5c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ comprehend_model_id + '/_deploy'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49b7d6c",
   "metadata": {},
   "source": [
    "#### Step 11. Test the Comprehend model through OpenSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0fe342",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ comprehend_model_id + '/_predict'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "    \"parameters\": {\n",
    "        \"Text\": \"你知道厕所在哪里吗\"\n",
    "    }\n",
    "}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca70cd",
   "metadata": {},
   "source": [
    "#### Step 12. Create the comprehend index pipeline\n",
    "\n",
    "Now we will create the pipeline for the index, this is how we tell OpenSearch to send the field(s) we wanted translations for to the Comprehend endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_ingest/pipeline/comprehend_language_identification_pipeline'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"description\": \"ingest reviews and identify lang with the comprehend model\",\n",
    "  \"processors\":[\n",
    "    {\n",
    "      \"ml_inference\": {\n",
    "        \"model_id\": comprehend_model_id,\n",
    "        \"input_map\": [\n",
    "            {\n",
    "               \"Text\": \"Text\"\n",
    "            }\n",
    "        ],\n",
    "        \"output_map\": [\n",
    "            {\n",
    "                \n",
    "            \"detected_language\": \"response.Languages[0].LanguageCode\",\n",
    "            \"language_score\": \"response.Languages[0].Score\"\n",
    "            }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f153a9e5",
   "metadata": {},
   "source": [
    "#### Step 13. Create the Comprehend index & test\n",
    "\n",
    "Next we create the index using the pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a72f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "index_name = 'comprehend_lang_ident_test01'\n",
    "path = '/' + index_name\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"default_pipeline\": \"comprehend_language_identification_pipeline\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the PUT request to create the index\n",
    "url = host + path\n",
    "response = requests.put(url, auth=awsauth, json=index_settings, headers=headers)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549b1963",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'comprehend_lang_ident_test01'\n",
    "path = '/' + index_name + '/_doc/'\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Define the document to index\n",
    "document = {\n",
    "    \"Text\": \"parlez vous francais?\"\n",
    "}\n",
    "\n",
    "url = host + path\n",
    "response = requests.post(url, auth=awsauth, json=document, headers=headers)\n",
    "\n",
    "\n",
    "print(response.json())\n",
    "\n",
    "doc_id = response.json()['_id']\n",
    "\n",
    "# Retrieve the indexed document\n",
    "get_path = '/' + index_name + '/_doc/' + doc_id\n",
    "get_url = host + get_path\n",
    "get_response = requests.get(get_url, auth=awsauth, headers=headers)\n",
    "\n",
    "# Print the retrieved document\n",
    "print(get_response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fd32ae",
   "metadata": {},
   "source": [
    "## Fasttext Language Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f8761f",
   "metadata": {},
   "source": [
    "Now that we've tested our Amazon Comprehend model connector, let's create a connector for our Sagemaker model using FastText language identification supported by BlazingText\n",
    "\n",
    "More information about this algorithm can be found here:https://docs.aws.amazon.com/sagemaker/latest/dg/blazingtext.html\n",
    "\n",
    "BlazingText is a GPU-accelerated implementation of FastText, capable of hosting pre-trained Text Classification and Word2Vec models, including FastText models. FastText is a neural network model used for both unsupervised word embedding generation and supervised text classification.\n",
    "\n",
    "While BlazingText employs custom CUDA kernels to speed up FastText's training process, the core algorithm remains the same for both. This compatibility allows users to leverage BlazingText's hosting capabilities on Amazon SageMaker for real-time predictions using FastText models. This is particularly useful if you have your own FastText-trained model or if one of the pre-trained models provided by the FastText team meets your requirements.\n",
    "\n",
    "In essence, BlazingText offers a way to deploy FastText models on SageMaker endpoints, combining the benefits of FastText's versatility with the computational efficiency of GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8247dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "import os\n",
    "import os.path\n",
    "import tarfile\n",
    "import sagemaker\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "region_name = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"blazingtext\", region=region_name)\n",
    "print('Using SageMaker BlazingText container: {} ({})'.format(container, region_name))\n",
    "\n",
    "!wget -O model.bin https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.bin\n",
    "# This creates a tar file fromt he model and uploads it into our S3 bucket we created in Step 3. to store models\n",
    "!tar -czvf langid.tar.gz model.bin\n",
    "model_location = sess.upload_data(\"langid.tar.gz\", bucket=s3BucketName, key_prefix='custom_inference/fasttext-language-identification')\n",
    "!rm langid.tar.gz model.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7b460",
   "metadata": {},
   "source": [
    "#### Step 1. Deploy the fasttext model\n",
    "\n",
    "deploy the model as a SageMaker endpoint.\n",
    "\n",
    "*note: takes a few minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703181e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "from sagemaker.predictor import Predictor\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = sageMakerExecutionRoleArn\n",
    "    \n",
    "model_location = 's3://' + s3BucketName + '/custom_inference/fasttext-language-identification/langid.tar.gz'\n",
    "\n",
    "container = sagemaker.image_uris.retrieve(\"blazingtext\", region=region_name)\n",
    "\n",
    "\n",
    "lang_id = sagemaker.Model(model_data=model_location, image_uri=container, role=role, sagemaker_session=sess)\n",
    "endpoint_name = 'fasttext2'\n",
    "lang_id.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge',endpoint_name=endpoint_name)\n",
    "\n",
    "FT_predictor = Predictor(endpoint_name=endpoint_name, sagemaker_session=sess)\n",
    "FT_predictor.serializer = sagemaker.serializers.JSONSerializer()\n",
    "FT_predictor.deserializer = sagemaker.deserializers.JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e14335",
   "metadata": {},
   "source": [
    "#### Step 2. Test the fasttext endpoint\n",
    "\n",
    "Now we will test the newly created endpoint to see it gives us accurate language identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b131ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\"hi which language is this?\",\n",
    "             \"mon nom est Pierre\",\n",
    "             \"Dem Jungen gab ich einen Ball.\",\n",
    "             \"আমি বাড়ি যাবো.\"]\n",
    "payload = {\"instances\" : sentences}\n",
    "\n",
    "predictions = FT_predictor.predict(payload)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb246c0",
   "metadata": {},
   "source": [
    "#### Step 3. Create the connector for the fasttext model\n",
    "\n",
    "Now we will create the connector and model for the fasttext model through Amazon Opensearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb71335",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/connectors/_create'\n",
    "url = host + path\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "payload = {\n",
    "  \"name\": \"lang identification\",\n",
    "  \"description\": \"fasttext model\",\n",
    "  \"version\": 1,\n",
    "  \"protocol\": \"aws_sigv4\",\n",
    "  \"credential\": {\n",
    "    \"roleArn\": sageMakerOpenSearchRoleArn\n",
    "  },\n",
    "  \"parameters\": {\n",
    "    \"region\": \"us-east-1\",\n",
    "    \"service_name\": \"sagemaker\"\n",
    "  },\n",
    "  \"actions\": [\n",
    "    {\n",
    "      \"action_type\": \"predict\",\n",
    "      \"method\": \"POST\",\n",
    "      \"url\": \"https://runtime.sagemaker.us-east-1.amazonaws.com/endpoints/\" + FT_predictor.endpoint_name + \"/invocations\",\n",
    "      \"headers\": {\n",
    "        \"content-type\": \"application/json\"\n",
    "      },\n",
    "      \"request_body\": \"{ \\\"instances\\\": [ \\\"${parameters.text}\\\" ] }\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "ft_connector_response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "ft_connector = ft_connector_response.json()[\"connector_id\"]\n",
    "print('Connector id: ' + ft_connector)\n",
    "# print(comprehend_connector_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb64af1",
   "metadata": {},
   "source": [
    "#### Step 4. Register the fasttext model\n",
    "\n",
    "We now register the fasttext model to the model group and the connector that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bf95a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/_register'\n",
    "url = host + path\n",
    "\n",
    "payload = {\n",
    "    \"name\": \"fasttext\",\n",
    "    \"function_name\": \"remote\",\n",
    "    \"description\": \"lang id model\",\n",
    "    \"connector_id\": ft_connector\n",
    "}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "ft_model_id = response.json()['model_id']\n",
    "print('Model id: ' + ft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9982d4",
   "metadata": {},
   "source": [
    "#### Step 5. Deploy the fasttext model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d615dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ ft_model_id + '/_deploy'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "response = requests.post(url, auth=awsauth, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b54bdb",
   "metadata": {},
   "source": [
    "#### Step 6. Test the fasttext model through OpenSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a253808",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/_plugins/_ml/models/'+ ft_model_id + '/_predict'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"parameters\": {\n",
    "    \"text\": \"It's nice to see the flowers bloom and hear the birds sing in the spring\"\n",
    "  }\n",
    "}\n",
    "response = requests.post(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a6f5e9",
   "metadata": {},
   "source": [
    "#### Step 7 Create the fasttext index pipeline\n",
    "\n",
    "Now we will create the pipeline for the index, this is how we tell OpenSearch to send the field(s) we wanted translations for to the SageMaker endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc03a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "path = '/_ingest/pipeline/ft_language_identification_pipeline'\n",
    "url = host + path\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "payload = {\n",
    "  \"description\": \"ingest reviews and identify lang with the fasttext model via sagemaker endpoint\",\n",
    "  \"processors\":[\n",
    "    {\n",
    "      \"ml_inference\": {\n",
    "        \"model_id\": ft_model_id,\n",
    "        \"input_map\": [\n",
    "            {\n",
    "               \"text\": \"text\"\n",
    "            }\n",
    "        ],\n",
    "        \"output_map\": [\n",
    "            {\n",
    "                \"inference\":\"response[0].label\"\n",
    "            }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "response = requests.put(url, auth=awsauth, json=payload, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d77e02",
   "metadata": {},
   "source": [
    "#### Step 8. Create the fasttext index & test\n",
    "\n",
    "Next we create the index using the pipeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1faf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key,\n",
    "                   region, 'es', session_token=credentials.token)\n",
    "\n",
    "index_name = 'ft_lang_ident_test1'\n",
    "path = '/' + index_name\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "index_settings = {\n",
    "    \"settings\": {\n",
    "        \"index\": {\n",
    "            \"default_pipeline\": \"ft_language_identification_pipeline\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Send the PUT request to create the index\n",
    "url = host + path\n",
    "response = requests.put(url, auth=awsauth, json=index_settings, headers=headers)\n",
    "\n",
    "# Print the response\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf28c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index_name = 'ft_lang_ident_test1'\n",
    "path = '/' + index_name + '/_doc/'\n",
    "\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# Define the document to index\n",
    "document = {\n",
    "    \"text\": \"parlez vous francais?\"\n",
    "}\n",
    "\n",
    "url = host + path\n",
    "response = requests.post(url, auth=awsauth, json=document, headers=headers)\n",
    "\n",
    "\n",
    "print(response.json())\n",
    "\n",
    "doc_id = response.json()['_id']\n",
    "\n",
    "# Retrieve the indexed document\n",
    "get_path = '/' + index_name + '/_doc/' + doc_id\n",
    "get_url = host + get_path\n",
    "get_response = requests.get(get_url, auth=awsauth, headers=headers)\n",
    "\n",
    "# Print the retrieved document\n",
    "print(get_response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
